<!DOCTYPE html>
<html>
	<head>
		<meta name="author" content="reiver" />

		<title>Data Science For Software Engineers</title>

		<meta name="keywords" content="computational social science" />

		<meta name="DC.Date.Created" content="2013-03-26T07:47:10-07:00" />
	</head>
	<body>
		<p>
			<a href="http://www.hilarymason.com/">Hilary Mason</a> asked the question on Twitter:
		</p>
		<blockquote cite="http://twitter.com/hmason/status/313426603260669952">
			<a href="http://twitter.com/hmason/status/313426603260669952"><img src="hmason_tweet.png" alt="Data people -- what do you think software engineers should know about data science / machine learning?" /></a>
		</blockquote>
		<p>
			There were a number of answers to her question.
			All quoted:
		</p>
		<p>
			From <a href="http://twitter.com/el33th4xor">@el33th4xor</a>:
		</p>
		<blockquote cite="http://twitter.com/el33th4xor/status/313426962452463618">
			<p>
				Hi Hilary, I'm not exactly in that space, but I think the syllabus here is a good start:
				<a href="https://www.cs.cornell.edu/courses/cs4780/2012fa/">https://www.cs.cornell.edu/courses/cs4780/2012fa/</a>
			</p>
		</blockquote>
		<p>
			The syllabus mentioned is:
		</p>
		<blockquote cite="https://www.cs.cornell.edu/courses/cs4780/2012fa/">
			<p>
				Machine learning is concerned with the question of how to make computers learn from experience. The ability to learn is not only central to most aspects of intelligent behavior, but machine learning techniques have become key components of many software systems. For examples, machine learning techniques are used to create spam filters, to analyze customer purchase data, to understand natural language, or to detect fraudulent credit card transactions. 
			</p>
			<p>
				This course will introduce the fundamental set of techniques and algorithms that constitute machine learning as of today, ranging from classification methods like decision trees and support vector machines, over structured models like hidden Markov models, to clustering and matrix factorization methods for recommendation. The course will not only discuss individual algorithms and methods, but also tie principles and approaches together from a theoretical perspective. In particular, the course will cover the following topics:
			</p>
			<ul>
				<li>
					<b>Concept Learning</b> : Hypothesis space, version space 
				</li>
				<li>
					<b>Instance-based Learning</b> : K-Nearest Neighbors, collaborative filtering
				</li>
				<li>
					<b>Decision Trees</b> : TDIDT, attribute selection, pruning and overfitting
				</li>
				<li>
					<b>ML Experimentation</b> : Hypothesis tests, resampling estimates 
				</li>
				<li>
					<b>Linear Rules</b> : Perceptron, duality, mistake bound 
				</li>
				<li>
					<b>Support Vector Machines</b> : Optimal hyperplane, kernels, stability 
				</li>
				<li>
					<b>Generative Models</b> : Naïve Bayes, linear discriminant analysis 
				</li>
				<li>
					<b>Hidden Markov Models</b>: probabilistic model, estimation, Viterbi
				</li>
				<li>
					<b>Structured Output Prediction</b> : predicting sequences, rankings, etc.
				</li>
				<li>
					<b>Learning Theory</b> : PAC learning, mistake bounds, VC dimension 
				</li>
				<li>
					<b>Clustering</b> : HAC, k-means, mixture of Gaussians
				</li>
				<li>
					<b>Recommendation Systems</b>: Similarity based methods, matrix factorization
				</li>
			</ul>
		</blockquote>
		<p>
			From <a href="http://twitter.com/kumar303">@kumar303</a>:
		</p>
		<blockquote cite="http://twitter.com/kumar303/status/313427322143395840">
			<p>
				concurrency
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/techmilind">@techmilind</a>:
		</p>
		<blockquote cite="http://twitter.com/techmilind/status/313427586485198848">
			<p>
				communication patterns and working set sizes for popular ML algos, and interactivity/flexibility requirements for data science
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/honkfestival">@honkfestival</a>:
		</p>
		<blockquote cite="http://twitter.com/honkfestival/status/313427718651912192">
			<p>
				The importance of making evidence-based decisions whenever possible.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/esammer">@esammer</a>:
		</p>
		<blockquote cite="http://twitter.com/esammer/status/313427736469311489">
			<p>
				basic stats, probability, classification, techniques for exploration and description, basic viz. i'm math-dumb and those help me.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/gotoplanb">@gotoplanb</a>:
		</p>
		<blockquote cite="http://twitter.com/gotoplanb/status/313429800679251969">
			<p>
				Sampling error
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/jamesdotcuff">@jamesdotcuff</a>:
		</p>
		<blockquote cite="http://twitter.com/jamesdotcuff/status/313430221292457984">
			<p>
				being able to parse really messy input data - the algorithm is often cake in comparison ref: human genome ;-)
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/mdreid">@mdreid</a>:
		</p>
		<blockquote cite="http://twitter.com/mdreid/status/313430365601669120">
			<p>
				Conditional prob. (a.k.a Bayes' rule), bias/variance trade off, overfitting, x-validation, exploratory vs. confirmatory analysis.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/suzannTee">@suzannTee</a>:
		</p>
		<blockquote cite="http://twitter.com/suzannTee/status/313431125131403264">
			<p>
				the main types of learning algs, the intuition behind them, and the strengths and limitations of each in the context of REAL data.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/marcua">@marcua</a>:
		</p>
		<blockquote cite="http://twitter.com/marcua/status/313431965963522048">
			<p>
				when you can sample, and when you can't
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/StrictlyStat">@StrictlyStat</a>:
		</p>
		<blockquote cite="http://twitter.com/StrictlyStat/status/313432093592014848">
			<p>
				Understanding that the "data" may still transformations to make it a useful covariate - such as scaling per-unit/person measures.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/jmichaethompson">@jmichaethompson</a>:
		</p>
		<blockquote cite="http://twitter.com/jmichaethompson/status/313441915716636673">
			<p>
				Methods that trade-off accuracy for computational resources. That is a gateway drug to being a professional data nerd.
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/jmichaethompson/status/313444149254844417">
			<p>
				Also: Intuitive description of problem triage into classification, regression and forecasting.
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/jmichaethompson/status/313444722226130944">
			<p>
				Last thing: how summary statistics can lie to you if you don't look at your data:
				<a href="http://en.wikipedia.org/wiki/Anscombe%27s_quartet">http://en.wikipedia.org/wiki/Anscombe%27s_quartet</a>
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/statalgo">@statalgo</a>:
		</p>
		<blockquote cite="http://twitter.com/statalgo/status/313478753860845568">
			<p>
				I would stick to basics: what is a model (use pictures), linear regression/classification (only supervised), bias/variance tradeoff.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/j4cob">@j4cob</a>:
		</p>
		<blockquote cite="http://twitter.com/j4cob/status/313491917268733952">
			<p>
				confidence intervals.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/jrauser">@jrauser</a>:
		</p>
		<blockquote cite="http://twitter.com/jrauser/status/313528245159071744">
			<p>
				All quantities of interest have uncertainty/error. (Repeated) measuring reduces error. Error drops with the square root of N.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/snoble">@snoble</a>:
		</p>
		<blockquote cite="http://twitter.com/snoble/status/313543249128214528">
			<p>
				if they build their applications to not destroy or modify data they will have useful data when they are ready for data science
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/snoble/status/313544189235310592">
			<p>
				and the "code ages like fish, data like wine" adage is pretty good. SW engs need to not be too clever in how they capture data.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/ncoghlan_dev">@ncoghlan_dev</a>:
		</p>
		<blockquote cite="http://twitter.com/ncoghlan_dev/status/313559235948589056">
			<p>
				That data exploration and production applications use different priorities to govern the code creation process.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/fhuszar">@fhuszar</a>:
		</p>
		<blockquote cite="http://twitter.com/fhuszar/status/313580151160061952">
			<p>
				That there is difference between training and prediction: training doesn't always have to use billions of data. Also: X-validation.
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/fhuszar/status/313677744309415937">
			<p>
				Oh, and the fact that counting ~= machine learning.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/dorkitude">@dorkitude</a>:
		</p>
		<blockquote cite="http://twitter.com/dorkitude/status/313668300448079872">
			<p>
				they should know that it's more approachable than they are imagining it is
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/dorkitude/status/313668714954379265">
			<p>
				emergent vs. prescribed predictive models; described vs. prescribed schema'd; the power of denormalization
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/emilsit">@emilsit</a>:
		</p>
		<blockquote cite="http://twitter.com/emilsit/status/313829472430338050">
			<p>
				I’m not a data person really, but statistics and visualization techniques come to mind.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/neilkod/status/313998531977428992">@neilkod</a>:
		</p>
		<blockquote cite="http://twitter.com/neilkod/status/313998531977428992">
			<p>
				awk. Tons of code unnecessarily written every day because awk wasn't in someone's tool belt. I've seen it too many times
			</p>
		</blockquote>
	</body>
</html>

