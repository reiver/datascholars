<!DOCTYPE html>
<html>
	<head>
		<meta name="author" content="reiver" />

		<title>Video Lectures: Information Theory, Pattern Recognition, and Neural Networks, by David J. C. MacKay</title>

		<meta name="keywords" content="book, machine learning, statistics" />

		<meta name="DC.Date.Created" content="2013-04-19T06:15:44-07:00" />
	</head>
	<body>
		<p>
			<a href="http://www.inference.phy.cam.ac.uk/mackay/">David J.C. MacKay</a> has a number of
			<a href="http://videolectures.net/course_information_theory_pattern_recognition/">video lectures</a>
			available on:
			<a href="http://videolectures.net/course_information_theory_pattern_recognition/">Information Theory, Pattern Recognition, and Neural Networks</a>.
		</p>
		<p>
			Here is the complete list of video lectures:
		</p>
		<ul>
			<li>
				<a href="http://videolectures.net/mackay_course_01/">Lecture 1: Introduction to Information Theory</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_02/">Lecture 2: Entropy and Data Compression (I): Introduction to Compression, Information Theory and Entropy</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_03/">Lecture 3: Entropy and Data Compression (II): Shannon's Source Coding Theorem and the Bent Coin Lottery</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_04/">Lecture 4: Entropy and Data Compression (III): Shannon's Source Coding Theorem, Symbol Codes</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_05/">Lecture 5: Entropy and Data Compression (IV): Shannon's Source Coding Theorem, Symbol Codes and Arithmetic Coding</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_06/">Lecture 6: Noisy Channel Coding (I): Inference and Information Measures for Noisy Channels</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_07/">Lecture 7: Noisy Channel Coding (II): The Capacity of a Noisy Channel</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_08/">Lecture 8: Noisy Channel Coding (III): The Noisy-Channel Coding Theorem</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_09/">Lecture 9: A Noisy Channel Coding Gem, And An Introduction To Bayesian Inference (I)</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_10/">Lecture 10: An Introduction To Bayesian Inference (II): Inference Of Parameters And Models</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_11/">Lecture 11: Approximating Probability Distributions (I): Clustering As An Example Inference Problem</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_12/">Lecture 12: Approximating Probability Distributions (II): Monte Carlo Methods (I): Importance Sampling, Rejection Sampling, Gibbs Sampling, Metropolis Method</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_13/">Lecture 13: Approximating Probability Distributions (III): Monte Carlo Methods (II): Slice Sampling, Hybrid Monte Carlo, Over-relaxation, Exact Sampling </a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_14/">Lecture 14: Approximating Probability Distributions (IV): Variational Methods</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_15/">Lecture 15: Data Modelling With Neural Networks (I): Feedforward Networks: The Capacity Of A Single Neuron, Learning As Inference</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_16/">Lecture 16: Data Modelling With Neural Networks (II): Content-Addressable Memories And State-Of-The-Art Error-Correcting Codes</a>
			</li>
		</ul>
	</body>
</html>

