<!DOCTYPE html>
<html>
	<head>
		<meta name="author" content="reiver" />

		<title>Deep Learning using Support Vector Machines (Yichuan Tang)</title>

		<meta name="keywords" content="support vector machine" />

		<meta name="DC.Date.Created" content="2013-06-05T06:44:48-07:00" />
	</head>
	<body>
		<blockquote cite="http://arxiv.org/abs/1306.0239">
			<p>
				Recently, fully-connected and convolutional neural networks have been trained to reach state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics data.
				For classification tasks, much of these "deep learning" models employ the softmax activation functions to learn output labels in 1-of-K format.
				In this paper, we demonstrate a small but consistent advantage of replacing softmax layer with a linear support vector machine.
				Learning minimizes a margin-based loss instead of the cross-entropy loss.
				In almost all of the previous works, hidden representation of deep networks are first learned using supervised or unsupervised techniques, and then are fed into SVMs as inputs.
				In contrast to those models, we are proposing to train all layers of the deep networks by backpropagating gradients through the top level SVM, learning features of all layers.
				Our experiments show that simply replacing softmax with linear SVMs gives significant gains on datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1306.0239">arXiv:1306.0239</a> [cs.LG]
		</p>
	</body>
</html>

