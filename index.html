<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8" />
		<title>DataScholars - Data Science, Computer Science, Machine Learning, Artificial Intelligence, Computational Social Science, Data Mining, Analytics, Visualization</title>
		<link rel="stylesheet" href="3/normalize/normalize.css" />
		<link rel="stylesheet" href="screen.css" />
		<link rel="alternate" href="/feed/blog.rss" title="DataScholars" type="application/rss+xml" />

		<script src="../../../../../3/jquery/jquery-1.9.1.min.js"></script>
	</head>

	<body>
		<header>
			<h1><a href="http://datascholars.com/">DataScholars</a></h1>
			<nav>
				<ul>
					<li>
						<a href="/about.html">About</a>
					</li>
					<li>
						<a href="/subscribe.html">Subscribe</a>
					</li>
					<li>
						<a href="/sphere.html">Sphere</a>
					</li>
				</ul>
			</nav>
			<p class="blurb">
				A blog about
				<em>data science</em>,
				<em>computer science</em>,
				<em>machine learning</em>,
				<em>artificial intelligence</em>,
				<em>computational social science</em>,
				<em>data mining</em>,
				<em>analysis</em>,
				and
				<em>visualization</em>.
			</p>
		</header>






		<article>
			<div class="pubdate">
				<time datetime="2013-08-02T06:38:02-07:00">August 2, 2013</time>
			</div>
			<h1><a href="/post/2013/08/02/applied_data_science_book/">Free Book: Applied Data Science</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<img src="data/post/2013/08/02/applied_data_science_book/kandinsky.comp-8.jpg" align="right" style="width:20%;"/>
			I'm a fan of free books.
			So when I saw
			"<a href="http://columbia-applied-data-science.github.io/appdatasci.pdf">Applied Data Science</a>"
			by
			<a href="http://ianlangmore.com/">Ian Langmore</a>
			and
			<a href="http://notjustmath.wordpress.com/">Daniel Krasner</a>,
			it caught my attention.
		</p>
		<p>
			"<a href="http://columbia-applied-data-science.github.io/appdatasci.pdf">Applied Data Science</a>"
			is a free <em>data science</em> book that focuses more on the <em>statistics</em>  end of things,
			while also getting readers going on (basic) programming &amp; command line skills.
			It doesn't, however, really go into much of the stuff you would expect to see from the <em>machine learning</em> end of things.
			But that's OK; there are other book for that.
			And this book (<a href="http://columbia-applied-data-science.github.io/appdatasci.pdf">Applied Data Science</a>) is worth a read for the topics it <em>does</em> cover.
		</p>
		<p>
			<a href="http://columbia-applied-data-science.github.io/appdatasci.pdf">Download</a> the book here:
			<a href="http://columbia-applied-data-science.github.io/appdatasci.pdf">appdatasci.pdf</a>
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-07-30T21:51:23-07:00">July 30, 2013</time>
			</div>
			<h1><a href="post/2013/07/30/tommy_levi_data_science_slides/">Tommy Levi's &quot;Data Science in the Wild&quot; Slides</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			We had a record turn out at <a href="http://twitter.com/tslevi">Tommy Levi</a>'s <a href="http://datascholars.com/post/2013/06/25/tommy_levi_data_science/">Data Science in the Wild</a> talk.
			(At least 250 people showed up.)
		</p>
		<p>
			There has been a lot of people asking for Tommy's slides, so without further adieu, here are they are.
		</p>
		<p>
			<a rel="enclosure" href="../../../../../data/post/2013/07/30/tommy_levi_data_science_slides/tslevi-datascience-in-the-wild.pdf">Download</a> them here:
			<a rel="enclosure" href="../../../../../data/post/2013/07/30/tommy_levi_data_science_slides/tslevi-datascience-in-the-wild.pdf">tslevi-datascience-in-the-wild.pdf</a>
		</p>
		<p>
			If you want to do <em>data science</em> with R cluster, you want to read these slides.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-07-14T22:59:21-07:00">July 14, 2013</time>
			</div>
			<h1><a href="post/2013/07/14/yoshua_bengio_deep_learning/">Yoshua Bengio's AAAI Tutorial on Deep Learning of Representations</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://www.iro.umontreal.ca/~bengioy/talks/deep-learning-tutorial-aaai2013.html">Deep Learning of Representations: a AAAI 2013 Tutorial</a>
			is a tutorial <a href="http://www.iro.umontreal.ca/~bengioy">Yoshua Bengio</a> gave at the AAAI 2013.
		</p>
		<p>
			<a href="http://www.iro.umontreal.ca/~bengioy/talks/aaai2013-tutorial.pdf">Get <abbr title="Yoshua Bengio's">his</abbr> slides here</a>.
			And <a href="http://www.iro.umontreal.ca/~bengioy/talks/aaai2013-tutorial-refs.pdf">bibliographic references here</a>.
		</p>
		<p>
			(Via <a href="http://twitter.com/chris_brockett/status/356615204391485440">Chris Brockett</a>)
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-06-25T06:32:29-07:00">June 25, 2013</time>
			</div>
			<h1><a href="post/2013/06/25/tommy_levi_data_science/">Data Science in the Wild: Tommy Levi speaking at Vancouver Meetup on Wednesday June 26th at 6:30 PM</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://twitter.com/tslevi">Tommy Levi</a>, a data scientist in Vancouver,
			is giving an interesting talk on Wednesday June 26th at 6:30 PM.
		</p>
		<p>
			Tommy will be speaking at a combined meetup event for the 
			Vancouver-based <a href="http://www.meetup.com/DataScience/events/122946072/">Data Science group</a>,
			<a href="http://www.meetup.com/MachineLearning/events/122946682/">Machine Learning group</a>
			and the <a href="http://www.meetup.com/Vancouver-R-Users-Group-data-analysis-statistics/events/124317472/">Vancouver R user group</a>.
		</p>
		<p>
			(At the time of writing, over 200 people have registered for the talk.
			So there is obviously a lot of interest.)
		</p>
		<p>
			Here is Tommy's talk's abstract:
		</p>
		<blockquote>
			<p>
				<b>Analyzing User Behavior at Plenty of Fish: Data Science in the Wild</b>
			</p>
			<p>
				How is Machine Learning and Data Science actually used in the real-world?
			</p>
			<p>
				Tommy Levi tells you how, and goes into the details of how he has been using them.
			</p>
			<p>
				Tommy will walk through the opening steps (and missteps) he took in starting to analyze user behavior on the Plenty of Fish site.
				He will discuss data preparation and wrangling, parallel computing and initial exploration and feature analysis.
				The goal of the talk is not to focus on any specific algorithms, but to show the steps taken for a real world analysis on large, often messy data and how to get actionable, useful results from such an analysis.
			</p>
		</blockquote>
		<p>
			If you are in the Vancouver area, and are interested in Data Science or Machine Learning, you should be there.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-06-12T07:12:13-07:00">June 12, 2013</time>
			</div>
			<h1><a href="post/2013/06/12/recent_developments_in_deep_learning/">Recent Developments in Deep Learning</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://www.cs.toronto.edu/~hinton/">Geoff Hinton</a> is a well known name when it comes to artificial neural networks.
		</p>
		<p>
			Here Geoff Hinton talks about <a href="http://www.youtube.com/watch?v=vShMxxqtDDs">recent developments in deep learning</a>:
		</p>

		<figure>
			<iframe width="560" height="315" src="http://www.youtube.com/embed/vShMxxqtDDs" frameborder="0" allowfullscreen="true"></iframe>
			<figcaption>
				<b>Figure 1.</b>
				Geoff Hinton - Recent Developments in Deep Learning 
			</figcaption>
		</figure>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-06-05T06:44:48-07:00">June 5, 2013</time>
			</div>
			<h1><a href="post/2013/06/05/deep_learning_using_svm/">Deep Learning using Support Vector Machines (Yichuan Tang)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://arxiv.org/abs/1306.0239">
			<p>
				Recently, fully-connected and convolutional neural networks have been trained to reach state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics data.
				For classification tasks, much of these "deep learning" models employ the softmax activation functions to learn output labels in 1-of-K format.
				In this paper, we demonstrate a small but consistent advantage of replacing softmax layer with a linear support vector machine.
				Learning minimizes a margin-based loss instead of the cross-entropy loss.
				In almost all of the previous works, hidden representation of deep networks are first learned using supervised or unsupervised techniques, and then are fed into SVMs as inputs.
				In contrast to those models, we are proposing to train all layers of the deep networks by backpropagating gradients through the top level SVM, learning features of all layers.
				Our experiments show that simply replacing softmax with linear SVMs gives significant gains on datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1306.0239">arXiv:1306.0239</a> [cs.LG]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-06-04T07:13:20-07:00">June 4, 2013</time>
			</div>
			<h1><a href="post/2013/06/04/bayesian_predicting_the_popularity_of_tweets/">A Bayesian Approach for Predicting the Popularity of Tweets (Tauhid Zaman, Emily B. Fox, Eric T. Bradlow)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://arxiv.org/abs/1304.6777">
			<p>
				We predict the popularity of short messages called tweets created in the micro-blogging site known as Twitter.
				We measure the popularity of a tweet by the time-series path of its retweets, which is when people forward the tweet to others.
				We develop a probabilistic model for the evolution of the retweets using a Bayesian approach, and form predictions using only observations on the retweet times and the local network or "graph" structure of the retweeters.
				We obtain good step ahead forecasts and predictions of the final total number of retweets even when only a small fraction (i.e. less than one tenth) of the retweet paths are observed.
				This translates to good predictions within a few minutes of a tweet being posted and has potential implications for understanding the spread of broader ideas, memes, or trends in social networks and also revenue models for both individuals who "sell tweets" and for those looking to monetize their reach.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1304.6777">arXiv:1304.6777</a> [cs.SI]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-31T07:59:52-07:00">May 31, 2013</time>
			</div>
			<h1><a href="post/2013/05/31/john_langford_on_machine_learning/">John Langford on Machine Learning</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://hunch.net/~jl/">John Langford</a> is a machine learning research scientist at Microsoft Research New York
			and the principal developer of <a href="http://hunch.net/~vw/">Vowpal Wabbi</a>.
		</p>
		<p>
			Here <a href="http://www.youtube.com/watch?v=FBXZvpvktGU">John Langford talks</a> about machine learning.
		</p>

		<figure>
			<iframe width="420" height="315" src="http://www.youtube.com/embed/FBXZvpvktGU" frameborder="0" allowfullscreen="true"></iframe>
			<figcaption>
				<b>Figure 1.</b>
				Machine Learning for Industry with Microsoft Research Lead Scientist
			</figcaption>
		</figure>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-30T07:35:06-07:00">May 30, 2013</time>
			</div>
			<h1><a href="post/2013/05/30/vw/">Vowpal Wabbit (VW): Fast Open Source Optimization</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://hunch.net/~vw/">Vowpal Wabbit</a> (or just <a href="http://hunch.net/~vw/">VW</a> for short) is an open source system designed to be a fast, scalable, useful learning algorithm, used for solving optimization problems.
		</p>
		<p>
			<a href="http://github.com/JohnLangford/vowpal_wabbit/wiki/Examples">Examples</a> are available <a href="http://github.com/JohnLangford/vowpal_wabbit/wiki/Examples">here</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-29T21:42:19-07:00">May 29, 2013</time>
			</div>
			<h1><a href="post/2013/05/29/item-based_collaborative_filtering_recommendation_algorithms/">Item-based Collaborative Filtering Recommendation Algorithms (Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://wwwconference.org/www10/cdrom/papers/519/index.html">
			<p>
				Recommender systems apply knowledge discovery techniques to the problem of making personalized recommendations for information, products or services during a live interaction.
				These systems, especially the k-nearest neighbor collaborative filtering based ones, are achieving widespread success on the Web.
				The tremendous growth in the amount of available information and the number of visitors to Web sites in recent years poses some key challenges for recommender systems.
				These are: producing high quality recommendations, performing many recommendations per second for millions of users and items and achieving high coverage in the face of data sparsity.
				In traditional collaborative filtering systems the amount of work increases with the number of participants in the system.
				New recommender system technologies are needed that can quickly produce high quality recommendations, even for very large-scale problems.
				To address these issues we have explored item-based collaborative filtering techniques.
				Item-based techniques first analyze the user-item matrix to identify relationships between different items, and then use these relationships to indirectly compute recommendations for users.
			</p>
			<p>
				In this paper we analyze different item-based recommendation generation algorithms.
				We look into different techniques for computing item-item similarities (e.g., item-item correlation vs. cosine similarities between item vectors) and different techniques for obtaining recommendations from them (e.g., weighted sum vs. regression model).
				Finally, we experimentally evaluate our results and compare them to the basic k-nearest neighbor approach.
				Our experiments suggest that item-based algorithms provide dramatically better performance than user-based algorithms, while at the same time providing better quality than the best available user-based algorithms.
			</p>
		</blockquote>
		<p>
			[<a href="http://wwwconference.org/www10/cdrom/papers/519/index.html">HTML</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-14T07:12:50-07:00">May 14, 2013</time>
			</div>
			<h1><a href="post/2013/05/14/bayesian_sparse_distributed_memory/">Approximating Bayesian inference with a sparse distributed memory system (Joshua T. Abbott, Jessica B. Hamrick, Thomas L. Griffiths)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://cocosci.berkeley.edu/tom/papers/ApproxInferenceWithSDM.pdf">
			<p>
				Probabilistic models of cognition have enjoyed recent success in explaining how people make inductive inferences.
				Yet, the difficult computations over structured representations that are often required by these models seem incompatible with the continuous and distributed nature of human minds.
				To reconcile this issue, and to understand the implications of constraints on probabilistic models, we take the approach of formalizing the mechanisms by which cognitive and neural processes could approximate Bayesian inference.
				Specifically, we show that an associative memory system using sparse, distributed representations can be reinterpreted as an importance sampler, a Monte Carlo method of approximating Bayesian inference.
				This capacity is illustrated through two case studies: a simple letter reconstruction task, and the classic problem of property induction.
				Broadly, our work demonstrates that probabilistic models can be implemented in a practical, distributed manner, and helps bridge the gap between algorithmic- and computational-level models of cognition.
			</p>
		</blockquote>
		<p>
			[<a href="http://cocosci.berkeley.edu/tom/papers/ApproxInferenceWithSDM.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-12T08:39:40-07:00">May 12, 2013</time>
			</div>
			<h1><a href="post/2013/05/12/convex_optimization/">Video Lectures: Convex Optimization, by Stephen Boyd</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			Convex Optimization has become more and more important to people researching machine learning.
		</p>
		<p>
			Stephen Boyd has a series of video lectures available on this topic.
		</p>
		<p>
			The video lectures are in two parts: "Convex Optimization I" and "Convex Optimization II".
			Here is the description of the "Convex Optimization I" sub-series:
		</p>
		<blockquote>
			<p>
				Convex Optimization I concentrates on recognizing and solving convex optimization problems that arise in engineering.
				Convex sets, functions, and optimization problems.
				Basics of convex analysis.
				Least-squares, linear and quadratic programs, semidefinite programming, minimax, extremal volume, and other problems.
				Optimality conditions, duality theory, theorems of alternative, and applications.
				Interior-point methods.
				Applications to signal processing, control, digital and analog circuit design, computational geometry, statistics, and mechanical engineering.
			</p>
		</blockquote>
		<p>
			And here is the description for the "Convex Optimization II" sub-series:
		</p>
		<blockquote>
			<p>
				This course introduces topics such as subgradient, cutting-plane, and ellipsoid methods.
				Decentralized convex optimization via primal and dual decomposition.
				Alternating projections.
				Exploiting problem structure in implementation.
				Convex relaxations of hard problems, and global optimization via branch &amp; bound.
				Robust optimization.
				Selected applications in areas such as control, circuit design, signal processing, and communications.
			</p>
		</blockquote>
		<p>
			All video below:
		</p>
		<ul>
			<li>
				<a href="http://www.youtube.com/watch?v=McLq1hEq3UY">Lecture 1: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=P3W_wFZ2kUo">Lecture 2: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=kcOodzDGV4c">Lecture 3: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=lEN2xvTTr0E">Lecture 4: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=Ry5i8DGZrJs">Lecture 5: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=-T9cloGG_80">Lecture 6: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=VxQ8VHm1Ci4">Lecture 7: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=FJVmflArCXc">Lecture 8: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=3Q9mMluX3Gw">Lecture 9: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=gH13lxieYFU">Lecture 10: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=GxK04B9SVg4">Lecture 11: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=mNzu42FrlHo">Lecture 12: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=FkPLteYMK40">Lecture 13: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=ZmvQ7GQ_gPg">Lecture 14: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=sTCtkkqrY8A">Lecture 15: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=Ap8LGbCVx4I">Lecture 16: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=StlHUwd_AgM">Lecture 17: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=oMRVDILkpUI">Lecture 18: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=HZW-9Ar0iVc">Lecture 19: Convex Optimization I</a>
			</li>
		</ul>
		<ul>
			<li>
				<a href="http://www.youtube.com/watch?v=U3lJAObbMFI">Lecture 1: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=ZniZaKCTktI">Lecture 2: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=B51GgGCHBRk">Lecture 3: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=kE3wtUaQzpA">Lecture 4: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=fhAFzmnFVqU">Lecture 5: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=N3vJOq5ZmKc">Lecture 6: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=t0MmgkV4YrA">Lecture 7: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=dLp2m9ae_MQ">Lecture 8: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=Kwli6FkYQYY">Lecture 9: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=rAGMDhz23Aw">Lecture 10: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=upMWYV7S1Y0">Lecture 11: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=cHVpwyYU_LY">Lecture 12: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=E4gl91l0l40">Lecture 13: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=QsfQAPdxeyw">Lecture 14: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=DsXzUU691ts">Lecture 15: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=1A734g96Npk">Lecture 16: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=TzY09ZfmOUI">Lecture 17: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=ORo5IU9a55s">Lecture 18: Convex Optimization II</a>
			</li>
		</ul>
		<p>
			A <a href="http://www.youtube.com/view_play_list?p=3940DD956CDF0622">play list</a> is available too.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-08T07:12:45-07:00">May 8, 2013</time>
			</div>
			<h1><a href="post/2013/05/08/stein_paradox_in_statistics/">Stein's Paradox in Statistics (Bradley Efron, Carl Morris)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://www-stat.stanford.edu/~ckirby/brad/other/Article1977.pdf">
			<p>
				Sometimes a mathematical result is strikingly contrary to generally held belief even though an obviously valid proof is given.
				Charles Stein of Stanford University discovered such as a paradox in statistics in 1955.
				His result undermined a century and a half of work on estimation theory, going back to Karl Friedrich Gauss and Adrien Marie Legendre.
				After a long period of resistance to Stein's ideas, punctuated by frequent and sometimes angry debate, the sense of paradox has diminished and Stein's ideas are being incorporated into applied and theoretical statistics.
			</p>
			<p>
				Stein's paradox concerns the use of observed averages to estimate unobserved quantities.
				Averaging is the second most basic process in statistics, the first being the simple act of counting.
			</p>
			<p>
				[...]
			</p>
			<p>
				The paradoxical element in Stein's result is that it sometimes contradicts this elementary law of statistical theory.
			</p>
		</blockquote>
		<p>
			[<a href="http://www-stat.stanford.edu/~ckirby/brad/other/Article1977.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-07T06:11:54-07:00">May 7, 2013</time>
			</div>
			<h1><a href="post/2013/05/07/hadley_alexander_wickham_vancouver/">TOMORROW: Hadley Alexander Wickham: Speaking at Vancouver Meetup on Wednesday May 8th at 7:00 PM</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			Tomorrow is the day.
		</p>
		<p>
			<a href="http://www.r-project.org/">R</a> users are likely to know the name:
			<a href="http://had.co.nz/">Hadley Alexander Wickham</a>.
		</p>
		<p>
			Hadley will be in Vancouver tomorrow, and will be speaking at a combined meetup event for the 
			Vancouver-based <a href="http://www.meetup.com/DataScience/events/114687772/">Data Science group</a>
			and the <a href="http://www.meetup.com/Vancouver-R-Users-Group-data-analysis-statistics/events/114670912/">Vancouver R user group</a>.
		</p>
		<p>
			If you are in the Vancouver area, use R, or are interested in statistics or visualization, be there tomorrow.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-06T07:23:46-07:00">May 6, 2013</time>
			</div>
			<h1><a href="post/2013/05/06/signals_and_systems/">Video Lectures: Signals and Systems, by Alan V. Oppenheim</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://www.rle.mit.edu/people/directory/alan-oppenheim/">Alan V. Oppenheim</a>
			has a number of <a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/">video lectures</a>
			on: <a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/">Signals and Systems</a> (up on MIT OpenCourseWare).
		</p>
		<p>
			 Here is the complete list of video lectures:
		</p>
		<ul>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-1-introduction">Lecture 1: Introduction</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-2-signals-and-systems-part-i">Lecture 2: Signals and Systems: Part I</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-3-signals-and-systems-part-ii">Lecture 3: Signals and Systems: Part II</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-4-convolution">Lecture 4: Convolution</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-5-properties-of-linear-time-invariant-systems">Lecture 5: Properties of Linear, Time-Invariant Systems</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-6-systems-represented-by-differential-equations">Lecture 6: Systems Represented by Differential Equations</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-7-continuous-time-fourier-series">Lecture 7: Continuous-Time Fourier Series</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-8-continuous-time-fourier-transform">Lecture 8: Continuous-Time Fourier Transform</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-9-fourier-transform-properties">Lecture 9: Fourier Transform Properties</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-10-discrete-time-fourier-series">Lecture 10: Discrete-Time Fourier Series</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-11-discrete-time-fourier-transform">Lecture 11: Discrete-Time Fourier Transform</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-12-filtering">Lecture 12: Filtering</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-13-continuous-time-modulation">Lecture 13: Continuous-Time Modulation</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-14-demonstration-of-amplitude-modulation">Lecture 14: Demonstration of Amplitude Modulation</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-15-discrete-time-modulation">Lecture 15: Discrete-Time Modulation</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-16-sampling">Lecture 16: Sampling</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-17-interpolation">Lecture 17: Interpolation</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-18-discrete-time-processing-of-continuous-time-signals">Lecture 18: Discrete-Time Processing of Continuous-Time Signals</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-19-discrete-time-sampling">Lecture 19: Discrete-Time Sampling</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-20-the-laplace-transform">Lecture 20: The Laplace Transform</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-21-continuous-time-second-order-systems">Lecture 21: Continuous-Time Second-Order Systems</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-22-the-z-transform">Lecture 22: The z-Transform</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-23-mapping-continuous-time-filters-to-discrete-time-filters">Lecture 23: Mapping Continuous-Time Filters to Discrete-Time Filters</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-24-butterworth-filters">Lecture 24: Butterworth Filters</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-25-feedback">Lecture 25: Feedback</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-26-feedback-example-the-inverted-pendulum">Lecture 26: Feedback Example: The Inverted Pendulum</a>
			</li>
		</ul>
		<p>
			To get the most out of the video lectures, it would probably be a good idea to work through the
			<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/assignments">assignments</a>
			and
			<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/readings">readings</a> too.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-05T07:36:27-07:00">May 5, 2013</time>
			</div>
			<h1><a href="post/2013/05/05/nonlinear_dimensionality_reduction/">A Global Geometric Framework for Nonlinear Dimensionality Reduction (Joshua B. Tenenbaum, Vin de Silva, John C. Langford)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://www.robots.ox.ac.uk/~az/lectures/ml/tenenbaum-isomap-Science2000.pdf">
			<p>
				Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations.
				The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 10<sup>6</sup> optic nerve fibers—a manageably small number of perceptually relevant features.
				Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set.
				Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions.
				In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.
			</p>
		</blockquote>
		<p>
			[<a href="http://www.robots.ox.ac.uk/~az/lectures/ml/tenenbaum-isomap-Science2000.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<div class="pagination">
			<nav>
				<ul>
					<li>
						<a href="page/by10/shift0/4.html">Older Page &#8827;</a>
					</li>
				</ul>
			<nav>
		</div>

		<footer>
			<a href="http://twitter.com/DataScholars"><img src="i/twitter.png" alit="twitter"></a>
		</footer>


		<script src="3/jquery/jquery-1.9.1.min.js"></script>
		<script src="3/typeset/src/linked-list.js"></script>
		<script src="3/typeset/src/linebreak.js"></script>
		<script src="3/typeset/lib/hypher.js"></script>
		<script src="3/typeset/lib/en-us.js"></script>

		<script src="3/d3/d3.v3.min.js"></script>
		<script src="3/jsnetworkx/jsnetworkx.js"></script>

		<script src="webstats.js"></script>
	</body>
</html>
