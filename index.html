<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8" />
		<title>DataScholars - Data Science, Computer Science, Machine Learning, Artificial Intelligence, Computational Social Science, Data Mining, Analytics, Visualization</title>
		<link rel="stylesheet" href="3/normalize/normalize.css" />
		<link rel="stylesheet" href="screen.css" />
		<link rel="alternate" href="/feed/blog.rss" title="DataScholars" type="application/rss+xml" />

		<script src="../../../../../3/jquery/jquery-1.9.1.min.js"></script>
	</head>

	<body>
		<header>
			<h1><a href="http://datascholars.com/">DataScholars</a></h1>
			<nav>
				<ul>
					<li>
						<a href="/about.html">About</a>
					</li>
					<li>
						<a href="/subscribe.html">Subscribe</a>
					</li>
					<li>
						<a href="/sphere.html">Sphere</a>
					</li>
				</ul>
			</nav>
			<p class="blurb">
				A blog about
				<em>data science</em>,
				<em>computer science</em>,
				<em>machine learning</em>,
				<em>artificial intelligence</em>,
				<em>computational social science</em>,
				<em>data mining</em>,
				<em>analysis</em>,
				and
				<em>visualization</em>.
			</p>
		</header>






		<article>
			<div class="pubdate">
				<time datetime="2013-06-25T06:32:29-07:00">June 25, 2013</time>
			</div>
			<h1><a href="post/2013/06/25/tommy_levi_data_science/">Data Science in the Wild: Tommy Levi speaking at Vancouver Meetup on Wednesday June 26th at 6:30 PM</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://twitter.com/tslevi">Tommy Levi</a>, a data scientist in Vancouver,
			is giving an interesting talk on Wednesday June 26th at 6:30 PM.
		</p>
		<p>
			Tommy will be speaking at a combined meetup event for the 
			Vancouver-based <a href="http://www.meetup.com/DataScience/events/122946072/">Data Science group</a>,
			<a href="http://www.meetup.com/MachineLearning/events/122946682/">Machine Learning group</a>
			and the <a href="http://www.meetup.com/Vancouver-R-Users-Group-data-analysis-statistics/events/124317472/">Vancouver R user group</a>.
		</p>
		<p>
			(At the time of writing, over 200 people have registered for the talk.
			So there is obviously a lot of interest.)
		</p>
		<p>
			Here is Tommy's talk's abstract:
		</p>
		<blockquote>
			<p>
				<b>Analyzing User Behavior at Plenty of Fish: Data Science in the Wild</b>
			</p>
			<p>
				How is Machine Learning and Data Science actually used in the real-world?
			</p>
			<p>
				Tommy Levi tells you how, and goes into the details of how he has been using them.
			</p>
			<p>
				Tommy will walk through the opening steps (and missteps) he took in starting to analyze user behavior on the Plenty of Fish site.
				He will discuss data preparation and wrangling, parallel computing and initial exploration and feature analysis.
				The goal of the talk is not to focus on any specific algorithms, but to show the steps taken for a real world analysis on large, often messy data and how to get actionable, useful results from such an analysis.
			</p>
		</blockquote>
		<p>
			If you are in the Vancouver area, and are interested in Data Science or Machine Learning, you should be there.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-06-12T07:12:13-07:00">June 12, 2013</time>
			</div>
			<h1><a href="post/2013/06/12/recent_developments_in_deep_learning/">Recent Developments in Deep Learning</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://www.cs.toronto.edu/~hinton/">Geoff Hinton</a> is a well known name when it comes to artificial neural networks.
		</p>
		<p>
			Here Geoff Hinton talks about <a href="http://www.youtube.com/watch?v=vShMxxqtDDs">recent developments in deep learning</a>:
		</p>

		<figure>
			<iframe width="560" height="315" src="http://www.youtube.com/embed/vShMxxqtDDs" frameborder="0" allowfullscreen="true"></iframe>
			<figcaption>
				<b>Figure 1.</b>
				Geoff Hinton - Recent Developments in Deep Learning 
			</figcaption>
		</figure>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-06-05T06:44:48-07:00">June 5, 2013</time>
			</div>
			<h1><a href="post/2013/06/05/deep_learning_using_svm/">Deep Learning using Support Vector Machines (Yichuan Tang)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://arxiv.org/abs/1306.0239">
			<p>
				Recently, fully-connected and convolutional neural networks have been trained to reach state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics data.
				For classification tasks, much of these "deep learning" models employ the softmax activation functions to learn output labels in 1-of-K format.
				In this paper, we demonstrate a small but consistent advantage of replacing softmax layer with a linear support vector machine.
				Learning minimizes a margin-based loss instead of the cross-entropy loss.
				In almost all of the previous works, hidden representation of deep networks are first learned using supervised or unsupervised techniques, and then are fed into SVMs as inputs.
				In contrast to those models, we are proposing to train all layers of the deep networks by backpropagating gradients through the top level SVM, learning features of all layers.
				Our experiments show that simply replacing softmax with linear SVMs gives significant gains on datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1306.0239">arXiv:1306.0239</a> [cs.LG]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-06-04T07:13:20-07:00">June 4, 2013</time>
			</div>
			<h1><a href="post/2013/06/04/bayesian_predicting_the_popularity_of_tweets/">A Bayesian Approach for Predicting the Popularity of Tweets (Tauhid Zaman, Emily B. Fox, Eric T. Bradlow)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://arxiv.org/abs/1304.6777">
			<p>
				We predict the popularity of short messages called tweets created in the micro-blogging site known as Twitter.
				We measure the popularity of a tweet by the time-series path of its retweets, which is when people forward the tweet to others.
				We develop a probabilistic model for the evolution of the retweets using a Bayesian approach, and form predictions using only observations on the retweet times and the local network or "graph" structure of the retweeters.
				We obtain good step ahead forecasts and predictions of the final total number of retweets even when only a small fraction (i.e. less than one tenth) of the retweet paths are observed.
				This translates to good predictions within a few minutes of a tweet being posted and has potential implications for understanding the spread of broader ideas, memes, or trends in social networks and also revenue models for both individuals who "sell tweets" and for those looking to monetize their reach.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1304.6777">arXiv:1304.6777</a> [cs.SI]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-31T07:59:52-07:00">May 31, 2013</time>
			</div>
			<h1><a href="post/2013/05/31/john_langford_on_machine_learning/">John Langford on Machine Learning</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://hunch.net/~jl/">John Langford</a> is a machine learning research scientist at Microsoft Research New York
			and the principal developer of <a href="http://hunch.net/~vw/">Vowpal Wabbi</a>.
		</p>
		<p>
			Here <a href="http://www.youtube.com/watch?v=FBXZvpvktGU">John Langford talks</a> about machine learning.
		</p>

		<figure>
			<iframe width="420" height="315" src="http://www.youtube.com/embed/FBXZvpvktGU" frameborder="0" allowfullscreen="true"></iframe>
			<figcaption>
				<b>Figure 1.</b>
				Machine Learning for Industry with Microsoft Research Lead Scientist
			</figcaption>
		</figure>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-30T07:35:06-07:00">May 30, 2013</time>
			</div>
			<h1><a href="post/2013/05/30/vw/">Vowpal Wabbit (VW): Fast Open Source Optimization</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://hunch.net/~vw/">Vowpal Wabbit</a> (or just <a href="http://hunch.net/~vw/">VW</a> for short) is an open source system designed to be a fast, scalable, useful learning algorithm, used for solving optimization problems.
		</p>
		<p>
			<a href="http://github.com/JohnLangford/vowpal_wabbit/wiki/Examples">Examples</a> are available <a href="http://github.com/JohnLangford/vowpal_wabbit/wiki/Examples">here</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-29T21:42:19-07:00">May 29, 2013</time>
			</div>
			<h1><a href="post/2013/05/29/item-based_collaborative_filtering_recommendation_algorithms/">Item-based Collaborative Filtering Recommendation Algorithms (Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://wwwconference.org/www10/cdrom/papers/519/index.html">
			<p>
				Recommender systems apply knowledge discovery techniques to the problem of making personalized recommendations for information, products or services during a live interaction.
				These systems, especially the k-nearest neighbor collaborative filtering based ones, are achieving widespread success on the Web.
				The tremendous growth in the amount of available information and the number of visitors to Web sites in recent years poses some key challenges for recommender systems.
				These are: producing high quality recommendations, performing many recommendations per second for millions of users and items and achieving high coverage in the face of data sparsity.
				In traditional collaborative filtering systems the amount of work increases with the number of participants in the system.
				New recommender system technologies are needed that can quickly produce high quality recommendations, even for very large-scale problems.
				To address these issues we have explored item-based collaborative filtering techniques.
				Item-based techniques first analyze the user-item matrix to identify relationships between different items, and then use these relationships to indirectly compute recommendations for users.
			</p>
			<p>
				In this paper we analyze different item-based recommendation generation algorithms.
				We look into different techniques for computing item-item similarities (e.g., item-item correlation vs. cosine similarities between item vectors) and different techniques for obtaining recommendations from them (e.g., weighted sum vs. regression model).
				Finally, we experimentally evaluate our results and compare them to the basic k-nearest neighbor approach.
				Our experiments suggest that item-based algorithms provide dramatically better performance than user-based algorithms, while at the same time providing better quality than the best available user-based algorithms.
			</p>
		</blockquote>
		<p>
			[<a href="http://wwwconference.org/www10/cdrom/papers/519/index.html">HTML</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-14T07:12:50-07:00">May 14, 2013</time>
			</div>
			<h1><a href="post/2013/05/14/bayesian_sparse_distributed_memory/">Approximating Bayesian inference with a sparse distributed memory system (Joshua T. Abbott, Jessica B. Hamrick, Thomas L. Griffiths)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://cocosci.berkeley.edu/tom/papers/ApproxInferenceWithSDM.pdf">
			<p>
				Probabilistic models of cognition have enjoyed recent success in explaining how people make inductive inferences.
				Yet, the difficult computations over structured representations that are often required by these models seem incompatible with the continuous and distributed nature of human minds.
				To reconcile this issue, and to understand the implications of constraints on probabilistic models, we take the approach of formalizing the mechanisms by which cognitive and neural processes could approximate Bayesian inference.
				Specifically, we show that an associative memory system using sparse, distributed representations can be reinterpreted as an importance sampler, a Monte Carlo method of approximating Bayesian inference.
				This capacity is illustrated through two case studies: a simple letter reconstruction task, and the classic problem of property induction.
				Broadly, our work demonstrates that probabilistic models can be implemented in a practical, distributed manner, and helps bridge the gap between algorithmic- and computational-level models of cognition.
			</p>
		</blockquote>
		<p>
			[<a href="http://cocosci.berkeley.edu/tom/papers/ApproxInferenceWithSDM.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-12T08:39:40-07:00">May 12, 2013</time>
			</div>
			<h1><a href="post/2013/05/12/convex_optimization/">Video Lectures: Convex Optimization, by Stephen Boyd</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			Convex Optimization has become more and more important to people researching machine learning.
		</p>
		<p>
			Stephen Boyd has a series of video lectures available on this topic.
		</p>
		<p>
			The video lectures are in two parts: "Convex Optimization I" and "Convex Optimization II".
			Here is the description of the "Convex Optimization I" sub-series:
		</p>
		<blockquote>
			<p>
				Convex Optimization I concentrates on recognizing and solving convex optimization problems that arise in engineering.
				Convex sets, functions, and optimization problems.
				Basics of convex analysis.
				Least-squares, linear and quadratic programs, semidefinite programming, minimax, extremal volume, and other problems.
				Optimality conditions, duality theory, theorems of alternative, and applications.
				Interior-point methods.
				Applications to signal processing, control, digital and analog circuit design, computational geometry, statistics, and mechanical engineering.
			</p>
		</blockquote>
		<p>
			And here is the description for the "Convex Optimization II" sub-series:
		</p>
		<blockquote>
			<p>
				This course introduces topics such as subgradient, cutting-plane, and ellipsoid methods.
				Decentralized convex optimization via primal and dual decomposition.
				Alternating projections.
				Exploiting problem structure in implementation.
				Convex relaxations of hard problems, and global optimization via branch &amp; bound.
				Robust optimization.
				Selected applications in areas such as control, circuit design, signal processing, and communications.
			</p>
		</blockquote>
		<p>
			All video below:
		</p>
		<ul>
			<li>
				<a href="http://www.youtube.com/watch?v=McLq1hEq3UY">Lecture 1: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=P3W_wFZ2kUo">Lecture 2: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=kcOodzDGV4c">Lecture 3: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=lEN2xvTTr0E">Lecture 4: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=Ry5i8DGZrJs">Lecture 5: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=-T9cloGG_80">Lecture 6: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=VxQ8VHm1Ci4">Lecture 7: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=FJVmflArCXc">Lecture 8: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=3Q9mMluX3Gw">Lecture 9: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=gH13lxieYFU">Lecture 10: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=GxK04B9SVg4">Lecture 11: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=mNzu42FrlHo">Lecture 12: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=FkPLteYMK40">Lecture 13: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=ZmvQ7GQ_gPg">Lecture 14: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=sTCtkkqrY8A">Lecture 15: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=Ap8LGbCVx4I">Lecture 16: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=StlHUwd_AgM">Lecture 17: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=oMRVDILkpUI">Lecture 18: Convex Optimization I</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=HZW-9Ar0iVc">Lecture 19: Convex Optimization I</a>
			</li>
		</ul>
		<ul>
			<li>
				<a href="http://www.youtube.com/watch?v=U3lJAObbMFI">Lecture 1: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=ZniZaKCTktI">Lecture 2: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=B51GgGCHBRk">Lecture 3: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=kE3wtUaQzpA">Lecture 4: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=fhAFzmnFVqU">Lecture 5: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=N3vJOq5ZmKc">Lecture 6: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=t0MmgkV4YrA">Lecture 7: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=dLp2m9ae_MQ">Lecture 8: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=Kwli6FkYQYY">Lecture 9: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=rAGMDhz23Aw">Lecture 10: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=upMWYV7S1Y0">Lecture 11: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=cHVpwyYU_LY">Lecture 12: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=E4gl91l0l40">Lecture 13: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=QsfQAPdxeyw">Lecture 14: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=DsXzUU691ts">Lecture 15: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=1A734g96Npk">Lecture 16: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=TzY09ZfmOUI">Lecture 17: Convex Optimization II</a>
			</li>
			<li>
				<a href="http://www.youtube.com/watch?v=ORo5IU9a55s">Lecture 18: Convex Optimization II</a>
			</li>
		</ul>
		<p>
			A <a href="http://www.youtube.com/view_play_list?p=3940DD956CDF0622">play list</a> is available too.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-08T07:12:45-07:00">May 8, 2013</time>
			</div>
			<h1><a href="post/2013/05/08/stein_paradox_in_statistics/">Stein's Paradox in Statistics (Bradley Efron, Carl Morris)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://www-stat.stanford.edu/~ckirby/brad/other/Article1977.pdf">
			<p>
				Sometimes a mathematical result is strikingly contrary to generally held belief even though an obviously valid proof is given.
				Charles Stein of Stanford University discovered such as a paradox in statistics in 1955.
				His result undermined a century and a half of work on estimation theory, going back to Karl Friedrich Gauss and Adrien Marie Legendre.
				After a long period of resistance to Stein's ideas, punctuated by frequent and sometimes angry debate, the sense of paradox has diminished and Stein's ideas are being incorporated into applied and theoretical statistics.
			</p>
			<p>
				Stein's paradox concerns the use of observed averages to estimate unobserved quantities.
				Averaging is the second most basic process in statistics, the first being the simple act of counting.
			</p>
			<p>
				[...]
			</p>
			<p>
				The paradoxical element in Stein's result is that it sometimes contradicts this elementary law of statistical theory.
			</p>
		</blockquote>
		<p>
			[<a href="http://www-stat.stanford.edu/~ckirby/brad/other/Article1977.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-07T06:11:54-07:00">May 7, 2013</time>
			</div>
			<h1><a href="post/2013/05/07/hadley_alexander_wickham_vancouver/">TOMORROW: Hadley Alexander Wickham: Speaking at Vancouver Meetup on Wednesday May 8th at 7:00 PM</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			Tomorrow is the day.
		</p>
		<p>
			<a href="http://www.r-project.org/">R</a> users are likely to know the name:
			<a href="http://had.co.nz/">Hadley Alexander Wickham</a>.
		</p>
		<p>
			Hadley will be in Vancouver tomorrow, and will be speaking at a combined meetup event for the 
			Vancouver-based <a href="http://www.meetup.com/DataScience/events/114687772/">Data Science group</a>
			and the <a href="http://www.meetup.com/Vancouver-R-Users-Group-data-analysis-statistics/events/114670912/">Vancouver R user group</a>.
		</p>
		<p>
			If you are in the Vancouver area, use R, or are interested in statistics or visualization, be there tomorrow.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-06T07:23:46-07:00">May 6, 2013</time>
			</div>
			<h1><a href="post/2013/05/06/signals_and_systems/">Video Lectures: Signals and Systems, by Alan V. Oppenheim</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://www.rle.mit.edu/people/directory/alan-oppenheim/">Alan V. Oppenheim</a>
			has a number of <a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/">video lectures</a>
			on: <a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/">Signals and Systems</a> (up on MIT OpenCourseWare).
		</p>
		<p>
			 Here is the complete list of video lectures:
		</p>
		<ul>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-1-introduction">Lecture 1: Introduction</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-2-signals-and-systems-part-i">Lecture 2: Signals and Systems: Part I</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-3-signals-and-systems-part-ii">Lecture 3: Signals and Systems: Part II</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-4-convolution">Lecture 4: Convolution</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-5-properties-of-linear-time-invariant-systems">Lecture 5: Properties of Linear, Time-Invariant Systems</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-6-systems-represented-by-differential-equations">Lecture 6: Systems Represented by Differential Equations</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-7-continuous-time-fourier-series">Lecture 7: Continuous-Time Fourier Series</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-8-continuous-time-fourier-transform">Lecture 8: Continuous-Time Fourier Transform</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-9-fourier-transform-properties">Lecture 9: Fourier Transform Properties</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-10-discrete-time-fourier-series">Lecture 10: Discrete-Time Fourier Series</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-11-discrete-time-fourier-transform">Lecture 11: Discrete-Time Fourier Transform</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-12-filtering">Lecture 12: Filtering</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-13-continuous-time-modulation">Lecture 13: Continuous-Time Modulation</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-14-demonstration-of-amplitude-modulation">Lecture 14: Demonstration of Amplitude Modulation</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-15-discrete-time-modulation">Lecture 15: Discrete-Time Modulation</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-16-sampling">Lecture 16: Sampling</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-17-interpolation">Lecture 17: Interpolation</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-18-discrete-time-processing-of-continuous-time-signals">Lecture 18: Discrete-Time Processing of Continuous-Time Signals</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-19-discrete-time-sampling">Lecture 19: Discrete-Time Sampling</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-20-the-laplace-transform">Lecture 20: The Laplace Transform</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-21-continuous-time-second-order-systems">Lecture 21: Continuous-Time Second-Order Systems</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-22-the-z-transform">Lecture 22: The z-Transform</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-23-mapping-continuous-time-filters-to-discrete-time-filters">Lecture 23: Mapping Continuous-Time Filters to Discrete-Time Filters</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-24-butterworth-filters">Lecture 24: Butterworth Filters</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-25-feedback">Lecture 25: Feedback</a>
			</li>
			<li>
				<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/video-lectures/lecture-26-feedback-example-the-inverted-pendulum">Lecture 26: Feedback Example: The Inverted Pendulum</a>
			</li>
		</ul>
		<p>
			To get the most out of the video lectures, it would probably be a good idea to work through the
			<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/assignments">assignments</a>
			and
			<a href="http://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/readings">readings</a> too.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-05T07:36:27-07:00">May 5, 2013</time>
			</div>
			<h1><a href="post/2013/05/05/nonlinear_dimensionality_reduction/">A Global Geometric Framework for Nonlinear Dimensionality Reduction (Joshua B. Tenenbaum, Vin de Silva, John C. Langford)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://www.robots.ox.ac.uk/~az/lectures/ml/tenenbaum-isomap-Science2000.pdf">
			<p>
				Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations.
				The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 10<sup>6</sup> optic nerve fibers—a manageably small number of perceptually relevant features.
				Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set.
				Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions.
				In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.
			</p>
		</blockquote>
		<p>
			[<a href="http://www.robots.ox.ac.uk/~az/lectures/ml/tenenbaum-isomap-Science2000.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-04T09:46:35-07:00">May 4, 2013</time>
			</div>
			<h1><a href="post/2013/05/04/revealing_social_networks/">Revealing social networks of spammers through spectral clustering (Kevin S. Xu, Mark Kliger, Yilun Chen, Peter J. Woolf, Alfred O. Hero III)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://arxiv.org/abs/1305.0051">
			<p>
				To date, most studies on spam have focused only on the spamming phase of the spam cycle and have ignored the harvesting phase, which consists of the mass acquisition of email addresses.
				It has been observed that spammers conceal their identity to a lesser degree in the harvesting phase, so it may be possible to gain new insights into spammers' behavior by studying the behavior of harvesters, which are individuals or bots that collect email addresses.
				In this paper, we reveal social networks of spammers by identifying communities of harvesters with high behavioral similarity using spectral clustering.
				The data analyzed was collected through Project Honey Pot, a distributed system for monitoring harvesting and spamming.
				Our main findings are (1) that most spammers either send only phishing emails or no phishing emails at all, (2) that most communities of spammers also send only phishing emails or no phishing emails at all, and (3) that several groups of spammers within communities exhibit coherent temporal behavior and have similar IP addresses.
				Our findings reveal some previously unknown behavior of spammers and suggest that there is indeed social structure between spammers to be discovered.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1305.0051">arXiv:1305.0051</a> [cs.SI]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-26T07:08:21-07:00">April 26, 2013</time>
			</div>
			<h1><a href="post/2013/04/26/jsnetworkx/">JSNetworkX: Visualizing Graphs On The Web Using JavaScript</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			Visualizing graphs (in the <em>graph theory</em> sense of the word) can be a challenge.
			Visualizing graphs on the web can be a even bigger challenge.
			Luckily there is <a href="http://felix-kling.de/JSNetworkX/">JSNetworkX</a>.
		</p>
		<p>
			JSNetworkX is an open source JavaScript library that makes visualizing graph data easy.
		</p>
		<p>
			JSNetwork is a port of the <a href="http://networkx.github.io/">NetworkX</a> graph visualization library 
			to JavaScript and the Web
			(using <a href="http://d3js.org/">D3</a>).
		</p>
		<p>
			Here is an example, as shown in <em>figure 1</em>.
		</p>
		<figure>
			<div id="jsnetworkx_20130426_1" style="border:1px solid black;height:220px;"></div>
			<script>
			$(document).ready(function() {

					var G = jsnx.Graph();
     
					G.add_nodes_from([1,2,3,4], {group:0});
					G.add_nodes_from([5,6,7], {group:1});
					G.add_nodes_from([8,9,10,11], {group:2});
     
					G.add_path([1,2,5,6,7,8,11]);
					G.add_edges_from([[1,3],[1,4],[3,4],[2,3],[2,4],[8,9],[8,10],[9,10],[11,10],[11,9]]);
     
					var color = d3.scale.category20();
					jsnx.draw(G, {
						element: '#jsnetworkx_20130426_1',
						layout_attr: {
							charge: -120,
							linkDistance: 20
						},
						node_attr: {
							r: 5,
							title: function(d) { return d.label;}
						},
						node_style: {
							fill: function(d) {
								return color(d.data.group);
							},
							stroke: 'none'
						},
						edge_style: {
						fill: '#999'
					}
				});

			});
			</script>
			<figcaption>
				<b>Figure 1.</b>
				Sample graph visualization created with JSNetworkX.
			</figcaption>
		</figure>
		<p>
			And here is the code for the graph visualization in <em>figure 1</em>, as shown in <em>figure 2</em>.
		</p>
		<figure>
			<pre><code>
var G = jsnx.Graph();
     
G.add_nodes_from([1,2,3,4], {group:0});
G.add_nodes_from([5,6,7], {group:1});
G.add_nodes_from([8,9,10,11], {group:2});
     
G.add_path([1,2,5,6,7,8,11]);
G.add_edges_from([[1,3],[1,4],[3,4],[2,3],[2,4],[8,9],[8,10],[9,10],[11,10],[11,9]]);
     
var color = d3.scale.category20();
jsnx.draw(G, {
  element: '#PUT_IT_OF_WHERE_TO_RENDER_THE_GRAPH_HERE',
  layout_attr: {
    charge: -120,
    linkDistance: 20
  },
  node_attr: {
    r: 5,
    title: function(d) { return d.label;}
  },
  node_style: {
    fill: function(d) {
      return color(d.data.group);
    },
    stroke: 'none'
  },
  edge_style: {
    fill: '#999'
  }
});
			</code></pre>
			<figcaption>
				<b>Figure 2.</b>
				JavaScript source code using JSNetworkX for the graph visualization shown in <em>figure 1</em>.
			</figcaption>
		</figure>
		<p>
			More information about
			<a href="http://felix-kling.de/JSNetworkX/">JSNetworkX</a>
			is available
			<a href="http://felix-kling.de/JSNetworkX/">here</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-24T07:17:48-07:00">April 24, 2013</time>
			</div>
			<h1><a href="post/2013/04/24/networks_crowds_and_markets/">Networks, Crowds, and Markets: Reasoning About a Highly Connected World (David Easley, Jon Kleinberg)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<img src="../../../../../data/post/2013/04/24/networks_crowds_and_markets/networks_crowds_and_markets.jpg" align="right" style="width:240px;"/>
			<a href="http://www.arts.cornell.edu/econ/deasley/">David Easley</a>
			and
			<a href="http://www.cs.cornell.edu/home/kleinber/">Jon Kleinberg</a>
			have created an <i>excellent</i> book on (a subset of graph theory known as) small-world networks and their applications, such as social networks.
			Although that description probably doesn't do the book justice.
			(The book is definitely worth a read.)
		</p>
		<p>
			It is called:
			<a href="http://www.cs.cornell.edu/home/kleinber/networks-book/">Networks, Crowds, and Markets: Reasoning About a Highly Connected World</a>.
		</p>
		<p>
			You can either <a href="http://www.cambridge.org/us/networksbook">buy a copy here</a>
			or <a href="http://www.cs.cornell.edu/home/kleinber/networks-book/">download it for free</a>.
		</p>
		<p>
			Here is their description of it:
		</p>
		<blockquote cite="http://www.cs.cornell.edu/home/kleinber/networks-book/">
			<p>
				Over the past decade there has been a growing public fascination with the complex "connectedness" of modern society. This connectedness is found in many incarnations: in the rapid growth of the Internet and the Web, in the ease with which global communication now takes place, and in the ability of news and information as well as epidemics and financial crises to spread around the world with surprising speed and intensity. These are phenomena that involve networks, incentives, and the aggregate behavior of groups of people; they are based on the links that connect us and the ways in which each of our decisions can have subtle consequences for the outcomes of everyone else.
			</p>
			<p>
				<i>Networks, Crowds, and Markets</i> combines different scientific perspectives in its approach to understanding networks and behavior. Drawing on ideas from economics, sociology, computing and information science, and applied mathematics, it describes the emerging field of study that is growing at the interface of all these areas, addressing fundamental questions about how the social, economic, and technological worlds are connected.
			</p>
		</blockquote>
		<p>
			[<a href="http://www.cs.cornell.edu/home/kleinber/networks-book/">HTML + PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-22T07:28:02-07:00">April 22, 2013</time>
			</div>
			<h1><a href="post/2013/04/22/analysing_mood_patterns/">Analysing Mood Patterns in the United Kingdom through Twitter Content (Vasileios Lampos, Thomas Lansdall-Welfare, Ricardo Araya, Nello Cristianini)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://arxiv.org/abs/1304.5507">
			<p>
				Social Media offer a vast amount of geo-located and time-stamped textual content directly generated by people.
				This information can be analysed to obtain insights about the general state of a large population of users and to address scientific questions from a diversity of disciplines.
				In this work, we estimate temporal patterns of mood variation through the use of emotionally loaded words contained in Twitter messages, possibly reflecting underlying circadian and seasonal rhythms in the mood of the users.
				We present a method for computing mood scores from text using affective word taxonomies, and apply it to millions of tweets collected in the United Kingdom during the seasons of summer and winter.
				Our analysis results in the detection of strong and statistically significant circadian patterns for all the investigated mood types.
				Seasonal variation does not seem to register any important divergence in the signals, but a periodic oscillation within a 24-hour period is identified for each mood type.
				The main common characteristic for all emotions is their mid-morning peak, however their mood score patterns differ in the evenings.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1304.5507">arXiv:1304.5507</a> [cs.SI]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-21T09:45:23-07:00">April 21, 2013</time>
			</div>
			<h1><a href="post/2013/04/21/plos_text_mining_collection/">New: PLOS Text Mining</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			The folks over at <a href="http://www.plos.org/">PLOS</a> are introducing the <a href="http://www.ploscollections.org/textmining">PLOS Text Mining Collection</a>.
		</p>
		<blockquote cite="http://blogs.plos.org/everyone/2013/04/17/announcing-the-plos-text-mining-collection/">
			<p>
				Text Mining is an interdisciplinary field combining techniques from linguistics, computer science and statistics to build tools that can efficiently retrieve and extract information from digital text.
				Over the last few decades, there has been increasing interest in text mining research because of the potential commercial and academic benefits this technology might enable.
			</p>
			<p>
				[...]
			</p>
			<p>
				First, the rate of growth of the scientific literature has now outstripped the ability of individuals to keep pace with new publications, even in a restricted field of study.
				Second, text-mining tools have steadily increased in accuracy and sophistication to the point where they are now suitable for widespread application.
				Finally, the rapid increase in availability of digital text in an Open Access format now permits text-mining tools to be applied more freely than ever before.
			</p>
			<p>
				[...]
			</p>
			<p>
				PLOS launches the <a href="http://www.ploscollections.org/textmining">Text Mining Collection</a>, a compendium of major reviews and recent highlights published in the PLOS family of journals on the topic of text mining.
				As one of the major publishers of the Open Access scientific literature, it is perhaps no coincidence that research in text mining in PLOS journals is flourishing.
				As noted above, the widespread application and societal benefits of text mining is most easily achieved under an Open Access model of publishing, where the barriers to obtaining published articles are minimized and the ability to remix and redistribute data extracted from text is explicitly permitted.
				Furthermore, PLOS is one of the few publishers who is actively promoting text mining research by providing an open <a href="http://api.plos.org/">Application Programming Interface</a> to mine their journal content.
			</p>
		</blockquote>
		<p>
			See it <a href="http://www.ploscollections.org/textmining">here</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-19T06:15:44-07:00">April 19, 2013</time>
			</div>
			<h1><a href="post/2013/04/19/information_theory_pattern_recognition_and_neural_networks/">Video Lectures: Information Theory, Pattern Recognition, and Neural Networks, by David J. C. MacKay</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://www.inference.phy.cam.ac.uk/mackay/">David J.C. MacKay</a> has a number of
			<a href="http://videolectures.net/course_information_theory_pattern_recognition/">video lectures</a>
			available on:
			<a href="http://videolectures.net/course_information_theory_pattern_recognition/">Information Theory, Pattern Recognition, and Neural Networks</a>.
		</p>
		<p>
			Here is the complete list of video lectures:
		</p>
		<ul>
			<li>
				<a href="http://videolectures.net/mackay_course_01/">Lecture 1: Introduction to Information Theory</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_02/">Lecture 2: Entropy and Data Compression (I): Introduction to Compression, Information Theory and Entropy</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_03/">Lecture 3: Entropy and Data Compression (II): Shannon's Source Coding Theorem and the Bent Coin Lottery</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_04/">Lecture 4: Entropy and Data Compression (III): Shannon's Source Coding Theorem, Symbol Codes</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_05/">Lecture 5: Entropy and Data Compression (IV): Shannon's Source Coding Theorem, Symbol Codes and Arithmetic Coding</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_06/">Lecture 6: Noisy Channel Coding (I): Inference and Information Measures for Noisy Channels</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_07/">Lecture 7: Noisy Channel Coding (II): The Capacity of a Noisy Channel</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_08/">Lecture 8: Noisy Channel Coding (III): The Noisy-Channel Coding Theorem</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_09/">Lecture 9: A Noisy Channel Coding Gem, And An Introduction To Bayesian Inference (I)</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_10/">Lecture 10: An Introduction To Bayesian Inference (II): Inference Of Parameters And Models</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_11/">Lecture 11: Approximating Probability Distributions (I): Clustering As An Example Inference Problem</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_12/">Lecture 12: Approximating Probability Distributions (II): Monte Carlo Methods (I): Importance Sampling, Rejection Sampling, Gibbs Sampling, Metropolis Method</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_13/">Lecture 13: Approximating Probability Distributions (III): Monte Carlo Methods (II): Slice Sampling, Hybrid Monte Carlo, Over-relaxation, Exact Sampling </a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_14/">Lecture 14: Approximating Probability Distributions (IV): Variational Methods</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_15/">Lecture 15: Data Modelling With Neural Networks (I): Feedforward Networks: The Capacity Of A Single Neuron, Learning As Inference</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_16/">Lecture 16: Data Modelling With Neural Networks (II): Content-Addressable Memories And State-Of-The-Art Error-Correcting Codes</a>
			</li>
		</ul>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-18T07:02:36-07:00">April 18, 2013</time>
			</div>
			<h1><a href="post/2013/04/18/hadley_alexander_wickham_vancouver/">Hadley Alexander Wickham: Speaking at Vancouver Meetup on Wednesday May 8th at 7:00 PM</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			For those who use <a href="http://www.r-project.org/">R</a>, the name
			<a href="http://had.co.nz/">Hadley Alexander Wickham</a> is a well known one.
		</p>
		<p>
			Hadley is coming to Vancouver, and will be speaking at a combined meetup event for the 
			Vancouver-based <a href="http://www.meetup.com/DataScience/events/114687772/">Data Science group</a>
			and the <a href="http://www.meetup.com/Vancouver-R-Users-Group-data-analysis-statistics/events/114670912/">Vancouver R user group</a>.
		</p>
		<p>
			If you are in the Vancouver area, use R, or are interested in statistics or visualization, you should be there.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-17T07:27:54-07:00">April 17, 2013</time>
			</div>
			<h1><a href="post/2013/04/17/tools_for_exploring_data_and_models/">Practical Tools For Exploring Data And Models (Hadley Alexander Wickham)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://had.co.nz/thesis/">
			<p>
				This thesis describes three families of tools for exploring data and models.
				It is organised in roughly the same way that you perform a data analysis.
				First, you get the data in a form that you can work with.
				Chapter 2 describes the reshape framework for restructuring data.
				Second, you plot the data to get a feel for what is going on.
				Chapter 3 introduces the layered grammar of graphics.
				Third, you iterate between graphics and models to build a succinct quantitative summary of the data.
				Chapter 4 introduces some strategies for visualising models.
				Finally, you look back at what you have done, and contemplate what tools you need to do better in the future.
				Chapter 5 summarises the impact of my work and my plans for the future.
			</p>
		</blockquote>
		<p>
			[<a href="http://had.co.nz/thesis/practical-tools-hadley-wickham.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-16T07:04:17-0700">April 16, 2013</time>
			</div>
			<h1><a href="post/2013/04/16/spark_cli_graphs/">Sprak: Graph Visualization From The Command Line</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			So you are at the command line.
			You have a bunch of data you pulled from the database or a file.
			You did a bunch of "magic" to <code>awk</code>, <code>sort</code> and other command line tools to extract the "important" parts of the data.
		</p>
		<p>
			Now you want to see that "important" extract in a graph.
		</p>
		<p>
			What's the fastest way to visualize it?
			What's the fastest to see a graph?
		</p>
		<p>
			MS Excel?
			OpenOffice Calc?
			GNU Octave?
			R?
		</p>
		<p>
			No, no, no and no.
		</p>
		<p>
			The fastest way is using  <a href="http://zachholman.com/spark/">spark</a>.
		</p>
		<p>
			 <a href="http://zachholman.com/spark/">Spark</a>  lets you create and view graphs right from the command line.
		</p>
		<p>
			Here is a very basic example:
		</p>
		<figure>
			<figcaption>
				<pre><code>
spark 0 30 55 80 33 150
				</code></pre>
				<strong>Figure 1.</strong>
				Very basic usage of <code>spak</code>.
			</figcaption>
		</figure>
		<figure>
			<figcaption>
				<pre><code>
▁▂▃▅▂▇
				</code></pre>
				<strong>Figure 2.</strong>
				Output of <code>spark</code> command in <em>figure 1</em>.
			</figcaption>
		</figure>
		<p>
			And here is a more typical example:
		</p>
		<figure>
			<figcaption>
				<pre><code>
curl http://earthquake.usgs.gov/earthquakes/catalogs/eqs1day-M1.txt --silent | sed '1d' | cut -d, -f9 | spark
				</code></pre>
				<strong>Figure 3.</strong>
				More typical usage of <code>spak</code>.
				Magnitude of earthquakes over 1.0 in the last 24 hours.
			</figcaption>
		</figure>
		<figure>
			<figcaption>
				<pre><code>
 ▅▆▂▃▂▂▂▅▂▂▅▇▂▂▂▃▆▆▆▅▃▂▂▂▁▂▂▆▁▃▂▂▂▂▃▂▆
				</code></pre>
				<strong>Figure 4.</strong>
				Output of <code>spark</code> command in <em>figure 3</em>.
			</figcaption>
		</figure>
		<p>
			(<a href="https://github.com/holman/spark/wiki/Wicked-Cool-Usage">More spark examples here</a>.)
		</p>
		<p>
			Check out <a href="http://zachholman.com/spark/">spark</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-15T06:59:44-0700">April 15, 2013</time>
			</div>
			<h1><a href="post/2013/04/15/ontario_open_data/">We Love Open Data: Ontario Open Data</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			The "currency" of a <em>data scientist</em>'s vocation is (surprise, surprise) <em>data</em>.
			Sometimes data scientists have to go to great lengths to gather the data sets themselves.
			And sometimes people will "give" it to them. 
		</p>
		<p>
			The government of Ontario has done just that.
		</p>
		<p>
			Meet the <a href="http://www.ontario.ca/government/government-ontario-open-data">Ontario Open Data portal</a>.
		</p>
		<p>
			Explore the data sets they offer yourself.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-13T12:34:25-07:00">April 13, 2013</time>
			</div>
			<h1><a href="post/2013/04/13/strong_ties/">The emergence and role of strong ties in time-varying communication networks (Márton Karsai, Nicola Perra, Alessandro Vespignani)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://arxiv.org/abs/1303.5966">
			<p>
				In most social, information, and collaboration systems the complex activity of agents generates rapidly evolving time-varying networks.
				Temporal changes in the network structure and the dynamical processes occurring on its fabric are usually coupled in ways that still challenge our mathematical or computational modelling.
				Here we analyse a mobile call dataset describing the activity of millions of individuals and investigate the temporal evolution of their egocentric networks.
				We empirically observe a simple statistical law characterizing the memory of agents that quantitatively signals how much interactions are more likely to happen again on already established connections.
				We encode the observed dynamics in a reinforcement process defining a generative computational network model with time-varying connectivity patterns.
				This activity-driven network model spontaneously generates the basic dynamic process for the differentiation between strong and weak ties.
				The model is used to study the effect of time-varying heterogeneous interactions on the spreading of information on social networks.
				We observe that the presence of strong ties may severely inhibit the large scale spreading of information by confining the process among agents with recurrent communication patterns.
				Our results provide the counterintuitive evidence that strong ties may have a negative role in the spreading of information across networks.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1303.5966">arXiv:1303.5966</a> [physics.soc-ph]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-11T09:16:13-07:00">April 11, 2013</time>
			</div>
			<h1><a href="post/2013/04/11/node_centrality_in_weighted_networks/">Node Centrality in Weighted Networks: Generalizing Degree and Shortest Paths (Tore Opsahl, Filip Agneessens, John Skvoretz)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://toreopsahl.files.wordpress.com/2010/04/node_centrality_in_weighted_networks1.pdf">
			<p>
				Ties often have a strength naturally associated with them that differentiate them from each other.
				Tie strength has been operationalized as weights.
				A few network measures have been proposed for weighted networks, including three common measures of node centrality: degree, closeness, and betweenness.
				However, these generalizations have solely focused on tie weights, and not on the number of ties, which was the central component of the original measures.
				This paper proposes generalizations that combine both these aspects.
				We illustrate the benets of this approach by applying one of them to Freeman's EIES dataset.
			</p>
		</blockquote>
		<p>
			[<a href="http://toreopsahl.files.wordpress.com/2010/04/node_centrality_in_weighted_networks1.pdf">PDF</a>]
		</p>
		<p>
			Also see <a href="http://toreopsahl.com/2010/04/21/article-node-centrality-in-weighted-networks-generalizing-degree-and-shortest-paths/">article</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-10T09:35:40-07:00">April 10, 2013</time>
			</div>
			<h1><a href="post/2013/04/10/book_regret_analysis/">Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems (Sébastien Bubeck, Nicolò Cesa-Bianchi)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<img src="data/post/2013/04/10/book_regret_analysis/regret_analysis_of_stochastic_and_nonstochastic_multi-armed_bandit_problems.jpg" align="right" style="width:240px;"/>
			<a href="http://www.princeton.edu/~sbubeck/">Sébastien Bubeck</a> and <a href="http://homes.di.unimi.it/~cesabian/">Nicolò Cesa-Bianchi</a>
			have put out a book on <em>optimization</em> that will be of interest to many readers.
		</p>
		<p>
			It is called:
			<a href="http://www.princeton.edu/~sbubeck/SurveyBCB12.pdf">Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems</a>.
		</p>
		<p>
			You can either <a href="http://www.nowpublishers.com/product.aspx?product=MAL&amp;doi=2200000024">buy a copy here</a> (discount code: MAL022024)
			or <a href="http://www.princeton.edu/~sbubeck/SurveyBCB12.pdf">download it for free here</a>.
		</p>
		<p>
			Here's the authors' description:
		</p>
		<blockquote cite="http://www.princeton.edu/~sbubeck/SurveyBCB12.pdf">
			<p>
				Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration–exploitation trade-off.
				This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future.
				Although the study of bandit problems dates back to the 1930s, exploration–exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing.
				Mathematically, a multi-armed bandit is defined by the payoff process associated with each option.
				In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs.
				Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.
			</p>
		</blockquote>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-09T18:48:51-07:00">April 9, 2013</time>
			</div>
			<h1><a href="post/2013/04/09/polyglot_unconference_2013/">PolyGlot (Un)Conference 2013</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://polyglotconf.com/"><img src="data/post/2013/04/09/polyglot_unconference_2013/polyglot.png" align="right" style="width:200px;"/></a>
			The popular <a href="http://polyglotconf.com/">PolyGlot (Un)Conference</a> is happening on
			Friday May 24th to Sunday May 26th
			in Vancouver.
			<a href="http://polyglotconf.com/tickets/">Tickets are on sale</a> now.
		</p>
		<p>
			If you are in Vancouver, and you are a <em>software engineer</em> or <em>data scientist</em>, you should be there.
			(I'll be there.)
		</p>
		<p>
			Here's the schedule:
		</p>
		<ul>
			<li>
				Friday, May 24th, 2013: Tutorials
			</li>
			<li>
				Saturday, May 25th, 2013: Unconference
			</li>
			<li>
				Sunday, May 26th, 2013: Open Hack Day 
			</li>
		</ul>
		<p>
			If you can only make it one day, be there on <strong>Saturday, May 25th</strong>.
			(But obviously, the more days you can be there, the more you will get out of it.)
		</p>
		<p>
			There will be something on data science, machine learning and artificial intelligence there.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-08T08:38:03-07:00">April 8, 2013</time>
			</div>
			<h1><a href="post/2013/04/08/twitter_who_to_follow/">WTF: The Who to Follow Service at Twitter (Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, Reza Zadeh)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://www.stanford.edu/~rezab/papers/wtf_overview.pdf">
			<p>
				WTF ("Who to Follow") is Twitter's user recommendation service, which is responsible for creating millions of connections daily between users based on shared interests, common connections, and other related factors.
				This paper provides an architectural overview and shares lessons we learned in building and running the service over the past few years.
				Particularly noteworthy was our design decision to process the entire Twitter graph in memory on a single server, which signicantly reduced architectural complexity and allowed us to develop and deploy the service in only a few months.
				At the core of our architecture is Cassovary, an open-source in-memory graph processing engine we built from scratch for WTF.
				Besides powering Twitter's user recommendations, Cassovary is also used for search, discovery, promoted products, and other services as well.
				We describe and evaluate a few graph recommendation algorithms implemented in Cassovary, including a novel approach based on a combination of random walks and SALSA.
				Looking into the future, we revisit the design of our architecture and comment on its limitations, which are presently being addressed in a second-generation system under development.
			</p>
		</blockquote>
		<p>
			[<a href="http://www.stanford.edu/~rezab/papers/wtf_overview.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-07T08:40:32-07:00">April 7, 2013</time>
			</div>
			<h1><a href="post/2013/04/07/chaotic_boltzmann_machines/">Chaotic Boltzmann machines (Hideyuki Suzuki, Jun-ichi Imura, Yoshihiko Horio, Kazuyuki Aihara)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote>
			<p>
				The chaotic Boltzmann machine proposed in this paper is a chaotic pseudo-billiard system that works as a Boltzmann machine.
				Chaotic Boltzmann machines are shown numerically to have computing abilities comparable to conventional (stochastic) Boltzmann machines.
				Since no randomness is required, efficient hardware implementation is expected.
				Moreover, the ferromagnetic phase transition of the Ising model is shown to be characterised by the largest Lyapunov exponent of the proposed system.
				In general, a method to relate probabilistic models to nonlinear dynamics by derandomising Gibbs sampling is presented.
			</p>
		</blockquote>
		<p>
			<a href="http://dx.doi.org/10.1038/srep01610">10.1038/srep01610</a>
			<!-- http://www.nature.com/srep/2013/130405/srep01610/full/srep01610.html -->
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-05T06:57:47-07:00">April 5, 2013</time>
			</div>
			<h1><a href="post/2013/04/05/r_3_released/">R 3.0.0 Released</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://www.r-project.org/">R</a> is a popular environment for statistical computing and graphics.
			<a href="https://stat.ethz.ch/pipermail/r-announce/2013/000559.html">R version 3.0.0 has been released.</a>
		</p>
		<p>
			From the release e-mail:
		</p>
		<blockquote cite="https://stat.ethz.ch/pipermail/r-announce/2013/000559.html">
			<p>
				Major R releases have not previously marked great landslides in terms of new features. Rather, they represent that the codebase has developed to a new level of maturity. This is not going to be an exception to the rule.
			</p>
			<p>
				Version 1.0.0 was released at a point in time when we felt that we had reached a level of completeness and stability high enough to characterize a full statistical system, which could be put to production use. 
			</p>
			<p>
				Version 2.0.0 came out after strong enhancements of the memory management subsystem as well as several major features, including Sweave.
			</p>
			<p>
				Version 3.0.0, as of this writing, contains only really major new feature: The inclusion of long vectors (containing more than 2^31-1 elements!). More changes are likely to make it into the final release, but the main reason for having it as a new major release is that R over the last 8.5 years has reached a new level: we now have 64 bit support on all platforms, support for parallel processing, the Matrix package, and much more.
			</p>
		</blockquote>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-04T07:38:07-07:00">April 4, 2013</time>
			</div>
			<h1><a href="post/2013/04/04/josh_wills_data_scientist_definition/">Josh Wills: Data Scientist Definition</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			A definition of what a data scientist is from <a href="http://twitter.com/josh_wills">Josh Wills</a>:
		</p>
		<blockquote cite="http://twitter.com/josh_wills/status/198093512149958656">
			<a href="http://twitter.com/josh_wills/status/198093512149958656"><img src="data/post/2013/04/04/josh_wills_data_scientist_definition/josh_wills_tweet.png" alt="Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician."/></a>
		</blockquote>
		<p>
			(Sometimes I think <em>data engineer</em> might be a better name for it.)
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-03T07:14:42-07:00">April 3, 2013</time>
			</div>
			<h1><a href="post/2013/04/03/book_elements_of_statistical_learning/">The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Trevor Hastie, Robert Tibshirani, Jerome Friedman)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<img src="data/post/2013/04/03/book_elements_of_statistical_learning/CoverII_small.jpg" align="right" style="width:240px;"/>
			Statistics is a fundamental part of our vocation.
			<em>We live it and breathe it</em>, so to speak.
		</p>
		<p>
			Thus, interesting books on statistics tend to catch my attention.
			And when they are also freely available, even better :-)
			(Although with this book, you can also buy a copy of it, and help the authors out.)
		</p>
		<p>
			One such book that seems worth a read is:
			<a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</a>.
		</p>
		<p>
			The book is written by
			<a href="http://www-stat.stanford.edu/~hastie/">Trevor Hastie</a>,
			<a href="http://www-stat.stanford.edu/~tibs/">Robert Tibshirani</a>
			and
			<a href="http://www-stat.stanford.edu/~jhf">Jerome Friedman</a>,
			who have made the book <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/download.html">available both as a free download and for sale</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-02T06:11:56-07:00">April 2, 2013</time>
			</div>
			<h1><a href="post/2013/04/02/yelp_dataset_challenge/">Yelp Dataset Challenge; And A New Data Set To Play With</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			Yelp has made a new data set available with their <a href="http://www.yelp.com/dataset_challenge/">yelp dataset challenge</a>.
		</p>
		<p>
			Get it <a href="http://www.yelp.com/dataset_challenge/">here</a>.
		</p>
	



			</div>
		</article>







		<div class="pagination">
			<nav>
				<ul>
					<li>
						<a href="page/by10/shift0/1.html">Older Page &#8827;</a>
					</li>
				</ul>
			<nav>
		</div>

		<footer>
			<a href="http://twitter.com/DataScholars"><img src="i/twitter.png" alit="twitter"></a>
		</footer>


		<script src="3/jquery/jquery-1.9.1.min.js"></script>
		<script src="3/typeset/src/linked-list.js"></script>
		<script src="3/typeset/src/linebreak.js"></script>
		<script src="3/typeset/lib/hypher.js"></script>
		<script src="3/typeset/lib/en-us.js"></script>

		<script src="3/d3/d3.v3.min.js"></script>
		<script src="3/jsnetworkx/jsnetworkx.js"></script>

		<script src="webstats.js"></script>
	</body>
</html>
