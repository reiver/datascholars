<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8" />
		<title>DataScholars - Data Science, Computer Science, Machine Learning, Artificial Intelligence, Computational Social Science, Data Mining, Analytics, Visualization</title>
		<link rel="stylesheet" href="3/normalize/normalize.css" />
		<link rel="stylesheet" href="screen.css" />
		<link rel="alternate" href="/feed/blog.rss" title="DataScholars" type="application/rss+xml" />
	</head>

	<body>
		<header>
			<h1><a href="http://datascholars.com/">DataScholars</a></h1>
			<nav>
				<ul>
					<li>
						<a href="/about.html">About</a>
					</li>
					<li>
						<a href="/subscribe.html">Subscribe</a>
					</li>
					<li>
						<a href="/sphere.html">Sphere</a>
					</li>
				</ul>
			</nav>
			<p class="blurb">
				A blog about
				<em>data science</em>,
				<em>computer science</em>,
				<em>machine learning</em>,
				<em>artificial intelligence</em>,
				<em>computational social science</em>,
				<em>data mining</em>,
				<em>analysis</em>,
				and
				<em>visualization</em>.
			</p>
		</header>






		<article>
			<div class="pubdate">
				<time datetime="2013-03-29T09:32:55-07:00">March 29, 2013</time>
			</div>
			<h1><a href="post/2013/03/29/don_turnbull_data_science_yvr/">Don Turnbull's Data Science Talk Slides #datascienceyvr</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			If you were at the new <a href="http://datascholars.com/post/2013/03/11/data_science_meetup_vancouver/">Data Science meetup group</a> in Vancouver on <time datetime="2013-03-27">Wednesday</time>,
			then you saw a great introductory Data Science talk by <a href="http://donturn.com/">Don Turnbull</a> (<a href="http://twitter.com/donturn">@donturn</a>).
		</p>
		<p>
			He covered a lot!
			And I think every two slides he put up could have been expanded upon and turned into talks of their own! :-)
		</p>
		<p>
			I wanted to have my video camera ready to film it, but it didn't work out that way.
			But, even without that, Don has made the slides of his talk available.
		</p>
		<p>
			<a rel="enclosure" href="data/post/2013/03/29/don_turnbull_data_science_yvr/donturn-datascienceyvr.pdf">Download</a> them <a rel="enclosure" href="data/post/2013/03/29/don_turnbull_data_science_yvr/donturn-datascienceyvr.pdf">here</a>: 
			<a rel="enclosure" href="data/post/2013/03/29/don_turnbull_data_science_yvr/donturn-datascienceyvr.pdf">donturn-datascienceyvr.pdf</a>
		</p>
		<p>
			Also, if you are not "doing" data science yet and are wondering <em>what is data science</em>, Don's slides are a good place to start.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-28T07:48:21-07:00">March 28, 2013</time>
			</div>
			<h1><a href="post/2013/03/28/sentiment_analysis/">Unsupervised Sentiment Analysis with Emotional Signals (Xia Hu, Jiliang Tang, Huiji Gao, Huan Liu)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://www.public.asu.edu/~xiahu/papers/www13.pdf">
			<p>
				The explosion of social media services presents a great opportunity to understand the sentiment of the public via analyzing its large-scale and opinion-rich data.
				In social media, it is easy to amass vast quantities of unlabeled data, but very costly to obtain sentiment labels, which makes unsupervised sentiment analysis essential for various applications.
				It is challenging for traditional lexicon-based unsupervised methods due to the fact that expressions in social media are unstructured, informal, and fast-evolving.
				Emoticons and product ratings are examples of emotional signals that are associated with sentiments expressed in posts or words.
				Inspired by the wide availability of emotional signals in social media, we propose to study the problem of unsupervised <i>sentiment analysis</i> with <i>emotional signals</i>.
				In particular, we investigate whether the signals can potentially help sentiment analysis by providing a unifed way to model two main categories of emotional signals, i.e., emotion indication and emotion correlation.
				We further incorporate the signals into an unsupervised learning framework for sentiment analysis.
				In the experiment, we compare the proposed framework with the state-of-the-art methods on two Twitter datasets and empirically evaluate our proposed framework to gain a deep understanding of the effects of emotional signals.
			</p>
		</blockquote>
		<p>
			[<a href="http://www.public.asu.edu/~xiahu/papers/www13.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-27T07:32:48-07:00">March 27, 2013</time>
			</div>
			<h1><a href="post/2013/03/27/berkeley_computational_social_science/">New Berkeley Computational Social Science Lab</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			Berkeley has a new <em>computational social science</em> lab call <a href="http://dlab.berkeley.edu/">D-Lab</a>.
		</p>
		<p>
			Be sure to also follow them on Twitter at: <a href="http://twitter.com/DLabAtBerkeley">@DLabAtBerkeley</a>.
			And also follow their <a href="http://dlab.berkeley.edu/blog">blog</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-26T07:47:10-07:00">March 26, 2013</time>
			</div>
			<h1><a href="post/2013/03/26/data_science_for_software_engineers/">Data Science For Software Engineers</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://www.hilarymason.com/">Hilary Mason</a> asked the question on Twitter:
		</p>
		<blockquote cite="http://twitter.com/hmason/status/313426603260669952">
			<a href="http://twitter.com/hmason/status/313426603260669952"><img src="data/post/2013/03/26/data_science_for_software_engineers/hmason_tweet.png" alt="Data people -- what do you think software engineers should know about data science / machine learning?"/></a>
		</blockquote>
		<p>
			There were a number of answers to her question.
			All quoted:
		</p>
		<p>
			From <a href="http://twitter.com/el33th4xor">@el33th4xor</a>:
		</p>
		<blockquote cite="http://twitter.com/el33th4xor/status/313426962452463618">
			<p>
				Hi Hilary, I'm not exactly in that space, but I think the syllabus here is a good start:
				<a href="https://www.cs.cornell.edu/courses/cs4780/2012fa/">https://www.cs.cornell.edu/courses/cs4780/2012fa/</a>
			</p>
		</blockquote>
		<p>
			The syllabus mentioned is:
		</p>
		<blockquote cite="https://www.cs.cornell.edu/courses/cs4780/2012fa/">
			<p>
				Machine learning is concerned with the question of how to make computers learn from experience. The ability to learn is not only central to most aspects of intelligent behavior, but machine learning techniques have become key components of many software systems. For examples, machine learning techniques are used to create spam filters, to analyze customer purchase data, to understand natural language, or to detect fraudulent credit card transactions. 
			</p>
			<p>
				This course will introduce the fundamental set of techniques and algorithms that constitute machine learning as of today, ranging from classification methods like decision trees and support vector machines, over structured models like hidden Markov models, to clustering and matrix factorization methods for recommendation. The course will not only discuss individual algorithms and methods, but also tie principles and approaches together from a theoretical perspective. In particular, the course will cover the following topics:
			</p>
			<ul>
				<li>
					<b>Concept Learning</b> : Hypothesis space, version space 
				</li>
				<li>
					<b>Instance-based Learning</b> : K-Nearest Neighbors, collaborative filtering
				</li>
				<li>
					<b>Decision Trees</b> : TDIDT, attribute selection, pruning and overfitting
				</li>
				<li>
					<b>ML Experimentation</b> : Hypothesis tests, resampling estimates 
				</li>
				<li>
					<b>Linear Rules</b> : Perceptron, duality, mistake bound 
				</li>
				<li>
					<b>Support Vector Machines</b> : Optimal hyperplane, kernels, stability 
				</li>
				<li>
					<b>Generative Models</b> : Naïve Bayes, linear discriminant analysis 
				</li>
				<li>
					<b>Hidden Markov Models</b>: probabilistic model, estimation, Viterbi
				</li>
				<li>
					<b>Structured Output Prediction</b> : predicting sequences, rankings, etc.
				</li>
				<li>
					<b>Learning Theory</b> : PAC learning, mistake bounds, VC dimension 
				</li>
				<li>
					<b>Clustering</b> : HAC, k-means, mixture of Gaussians
				</li>
				<li>
					<b>Recommendation Systems</b>: Similarity based methods, matrix factorization
				</li>
			</ul>
		</blockquote>
		<p>
			From <a href="http://twitter.com/kumar303">@kumar303</a>:
		</p>
		<blockquote cite="http://twitter.com/kumar303/status/313427322143395840">
			<p>
				concurrency
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/techmilind">@techmilind</a>:
		</p>
		<blockquote cite="http://twitter.com/techmilind/status/313427586485198848">
			<p>
				communication patterns and working set sizes for popular ML algos, and interactivity/flexibility requirements for data science
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/honkfestival">@honkfestival</a>:
		</p>
		<blockquote cite="http://twitter.com/honkfestival/status/313427718651912192">
			<p>
				The importance of making evidence-based decisions whenever possible.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/esammer">@esammer</a>:
		</p>
		<blockquote cite="http://twitter.com/esammer/status/313427736469311489">
			<p>
				basic stats, probability, classification, techniques for exploration and description, basic viz. i'm math-dumb and those help me.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/gotoplanb">@gotoplanb</a>:
		</p>
		<blockquote cite="http://twitter.com/gotoplanb/status/313429800679251969">
			<p>
				Sampling error
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/jamesdotcuff">@jamesdotcuff</a>:
		</p>
		<blockquote cite="http://twitter.com/jamesdotcuff/status/313430221292457984">
			<p>
				being able to parse really messy input data - the algorithm is often cake in comparison ref: human genome ;-)
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/mdreid">@mdreid</a>:
		</p>
		<blockquote cite="http://twitter.com/mdreid/status/313430365601669120">
			<p>
				Conditional prob. (a.k.a Bayes' rule), bias/variance trade off, overfitting, x-validation, exploratory vs. confirmatory analysis.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/suzannTee">@suzannTee</a>:
		</p>
		<blockquote cite="http://twitter.com/suzannTee/status/313431125131403264">
			<p>
				the main types of learning algs, the intuition behind them, and the strengths and limitations of each in the context of REAL data.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/marcua">@marcua</a>:
		</p>
		<blockquote cite="http://twitter.com/marcua/status/313431965963522048">
			<p>
				when you can sample, and when you can't
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/StrictlyStat">@StrictlyStat</a>:
		</p>
		<blockquote cite="http://twitter.com/StrictlyStat/status/313432093592014848">
			<p>
				Understanding that the "data" may still transformations to make it a useful covariate - such as scaling per-unit/person measures.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/jmichaethompson">@jmichaethompson</a>:
		</p>
		<blockquote cite="http://twitter.com/jmichaethompson/status/313441915716636673">
			<p>
				Methods that trade-off accuracy for computational resources. That is a gateway drug to being a professional data nerd.
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/jmichaethompson/status/313444149254844417">
			<p>
				Also: Intuitive description of problem triage into classification, regression and forecasting.
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/jmichaethompson/status/313444722226130944">
			<p>
				Last thing: how summary statistics can lie to you if you don't look at your data:
				<a href="http://en.wikipedia.org/wiki/Anscombe%27s_quartet">http://en.wikipedia.org/wiki/Anscombe%27s_quartet</a>
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/statalgo">@statalgo</a>:
		</p>
		<blockquote cite="http://twitter.com/statalgo/status/313478753860845568">
			<p>
				I would stick to basics: what is a model (use pictures), linear regression/classification (only supervised), bias/variance tradeoff.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/j4cob">@j4cob</a>:
		</p>
		<blockquote cite="http://twitter.com/j4cob/status/313491917268733952">
			<p>
				confidence intervals.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/jrauser">@jrauser</a>:
		</p>
		<blockquote cite="http://twitter.com/jrauser/status/313528245159071744">
			<p>
				All quantities of interest have uncertainty/error. (Repeated) measuring reduces error. Error drops with the square root of N.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/snoble">@snoble</a>:
		</p>
		<blockquote cite="http://twitter.com/snoble/status/313543249128214528">
			<p>
				if they build their applications to not destroy or modify data they will have useful data when they are ready for data science
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/snoble/status/313544189235310592">
			<p>
				and the "code ages like fish, data like wine" adage is pretty good. SW engs need to not be too clever in how they capture data.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/ncoghlan_dev">@ncoghlan_dev</a>:
		</p>
		<blockquote cite="http://twitter.com/ncoghlan_dev/status/313559235948589056">
			<p>
				That data exploration and production applications use different priorities to govern the code creation process.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/fhuszar">@fhuszar</a>:
		</p>
		<blockquote cite="http://twitter.com/fhuszar/status/313580151160061952">
			<p>
				That there is difference between training and prediction: training doesn't always have to use billions of data. Also: X-validation.
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/fhuszar/status/313677744309415937">
			<p>
				Oh, and the fact that counting ~= machine learning.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/dorkitude">@dorkitude</a>:
		</p>
		<blockquote cite="http://twitter.com/dorkitude/status/313668300448079872">
			<p>
				they should know that it's more approachable than they are imagining it is
			</p>
		</blockquote>
		<blockquote cite="http://twitter.com/dorkitude/status/313668714954379265">
			<p>
				emergent vs. prescribed predictive models; described vs. prescribed schema'd; the power of denormalization
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/emilsit">@emilsit</a>:
		</p>
		<blockquote cite="http://twitter.com/emilsit/status/313829472430338050">
			<p>
				I’m not a data person really, but statistics and visualization techniques come to mind.
			</p>
		</blockquote>
		<p>
			From <a href="http://twitter.com/neilkod/status/313998531977428992">@neilkod</a>:
		</p>
		<blockquote cite="http://twitter.com/neilkod/status/313998531977428992">
			<p>
				awk. Tons of code unnecessarily written every day because awk wasn't in someone's tool belt. I've seen it too many times
			</p>
		</blockquote>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-25T06:57:15-07:00">March 25, 2013</time>
			</div>
			<h1><a href="post/2013/03/25/data_science_not_bi/">Data Science And Business Intelligence Not The Same Thing</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



			<p>
				Perhaps I am misunderstand the authors intend, but
				there is <a href="http://blogs.nature.com/naturejobs/2013/03/18/so-you-want-to-be-a-data-scientist">a blog post</a>
				on the <a href="http://blogs.nature.com/naturejobs/">Nature Jobs blog</a> that seems to be conflating
				<em>data science</em> with <em>business intelligence</em>.
				Here is an excerpt from the article:
			</p>
			<blockquote cite="http://blogs.nature.com/naturejobs/2013/03/18/so-you-want-to-be-a-data-scientist">
				<p>
					While programming and statistical expertise is the foundation for any data scientist, a strong background in business and strategy can help jettison a younger scientist's career to the next level.
				</p>
			</blockquote>
			<p>
				The article makes it sound like all <em>data scientists</em> do is <em>business intelligence</em>.
				That is false.
			</p>
			<p>
				Take fellow <a href="http:/datascholars.com">DataScholars</a> blogger and data scientist <a href="http://metadatascience.com/">Massoud Seifi</a> (a.k.a <a href="http://twitter.com/accesstoken">@accesstoken</a>).
				He spends his day as a <em>data scientist</em> dealing with questions on (online) identity and fraud detection &amp; prevention.
			</p>
			<p>
				As another example, I am aware of data science positions where the work is focused on computer vision problems.
				(Again, this has nothing to do with <em>business intelligence</em>.)
			</p>
			<p>
				Although I'm sure there are a number of <em>data scientists</em> who use their skills for <em>business intelligence</em>,
				many <em>data scientists</em> apply their <em>data science</em> skills towards problems that have absolutely nothing to do with <em>business intelligence</em>.
			</p>
			<p>
				For an analogy, it is like someone who has only ever seen software engineers working on iPhone apps; and then declares that all software engineers are iPhone developers.
				Obviously not true.
			</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-22T07:29:59-0700">March 22, 2013</time>
			</div>
			<h1><a href="post/2013/03/22/http_graph_gephi/">Visualize And Analyze The Web As You Browse</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="https://marketplace.gephi.org/plugin/http-graph/">HTTP Graph</a> is a <a href="http://gephi.org/">Gephi</a> plugin that lets you visualize and analyze the Web as you browse the web.
		</p>
		<p>
			<a href="http://vimeo.com/18591468">See it in action</a> in the following video.
		</p>
		<figure>
			<iframe src="http://player.vimeo.com/video/18591468" width="500" height="281" frameborder="0" webkitAllowFullScreen="1" mozallowfullscreen="1" allowFullScreen="1"></iframe>
			<figcaption>
				<strong>Figure 1.</strong>
				HTTP Graph - A Gephi plugin
			</figcaption>
		</figure>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-21T07:21:45-0700">March 21, 2013</time>
			</div>
			<h1><a href="post/2013/03/21/open_data_government_of_canada/">We Love Open Data: Open Data from Government of Canada</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			The "currency" of a <em>data scientist</em>'s vocation is (surprise, surprise) <em>data</em>.
			Sometimes data scientists have to go to great lengths to gather the data sets themselves.
			And sometimes people will "give" it to them. 
		</p>
		<p>
			The government of Canada has done just that.
		</p>
		<p>
			Meet the <a href="http://www.data.gc.ca/">Government of Canada Open Data portal</a>.
		</p>
		<p>
			Explore the data sets they offer yourself.
		</p>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-20T07:30:51-0700">March 20, 2013</time>
			</div>
			<h1><a href="post/2013/03/20/data_journalism_handbook/">Data Journalism Handbook</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			There are a lot of places where <em>data science</em> can be applied.
			Journalism is one of them.
		</p>
		<p>
			There is a free <a href="http://datajournalismhandbook.org/1.0/en/index.html">book</a> available online that encourages journalists to either start engaging in some limited <em>data science</em> techniques themselves or "connect" with people who can do it for them.
		</p>
		<p>
			The book is called the <a href="http://datajournalismhandbook.org/1.0/en/index.html">Data Journalism Handbook</a>.
			It is a quick read and can give you a bit of insight into how <em>data science</em> can get applied to <em>journalism</em>.
		</p>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-19T07:52:45-07:00">March 19, 2013</time>
			</div>
			<h1><a href="post/2013/03/19/going_viral_visualization/">Visualizing How Things Go Viral?</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://jakehofman.com/">Jake Hofman</a> talks a bit about <a href="http://www.youtube.com/watch?v=wSwOszoHuoI">visualizing how things go viral</a>.
		</p>
		<figure>
			<iframe width="560" height="315" src="http://www.youtube.com/embed/wSwOszoHuoI" frameborder="0" allowfullscreen="1"></iframe>
			<figcaption>
				<strong>Figure 1.</strong>
				ViralSearch
			</figcaption>
		</figure>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-18T07:21:44-07:00">March 18, 2013</time>
			</div>
			<h1><a href="post/2013/03/18/inferring_fb_creation_date_from_uid_data/">Lookup Table for Inferring Facebook Account Creation Date From Facebook User ID</a></h1>
			<address
				 
					class="reblogged"
				 
			>
				 
					<a rel="bookmark canonical" href="http://metadatascience.com/2013/03/14/lookup-table-for-inferring-facebook-account-creation-date-from-facebook-user-id/">cross-blogged</a>
				 
				by <a href="http://twitter.com/accesstoken">accesstoken</a>
			</address>
			<div>



		<p>
			In my <a href="http://metadatascience.com/2013/03/11/inferring-facebook-account-creation-date-from-facebook-user-id/">previous post</a>, I explained how we can estimate the account creation date of Facebook accounts that have a 15 digit UID without having to call the Facebook API and just based on the user’s Facebook UID.
		</p>
		<p>
			Table below shows the correlation between Facebook UID and Facebook Account Creation Date for the sample set that I analysed.
			The table is represented in <a href="http://en.wikipedia.org/wiki/Comma-separated_values">CSV</a> format as follows:
		</p>
		<p>
			<em>Facebook UID, Account Creation Date(timestamp), Account Creation Date(date)</em>.
		</p>
		<p>
			<strong>Note #1:</strong> To respect the users privacy I hided the last 5 digits of UIDs.
				You may replace ‘x’ by ‘0’ and it should not cause any problem.
		</p>
		<p>
			<strong>Note #2:</strong> For a more accurate result, this table should get updated.
		</p>
		<figure>
<pre><code>
1000053132xxxxx,1361330314,2013-02-19
1000049732xxxxx,1357606484,2013-01-07
1000047422xxxxx,1354021840,2012-11-27
1000047355xxxxx,1353417806,2012-11-20
1000046843xxxxx,1352662415,2012-11-11
1000046015xxxxx,1350999158,2012-10-23
1000035523xxxxx,1349467776,2012-10-05
1000040785xxxxx,1343717040,2012-07-30
1000041143xxxxx,1342928040,2012-07-21
1000038945xxxxx,1338526722,2012-05-31
1000036032xxxxx,1331873652,2012-03-15
1000031133xxxxx,1320583505,2011-11-06
1000031024xxxxx,1320464096,2011-11-04
1000029834xxxxx,1318571069,2011-10-13
1000029345xxxxx,1315974235,2011-09-13
1000026042xxxxx,1309652553,2011-07-02
1000023280xxxxx,1306728328,2011-05-29
1000024582xxxxx,1304995827,2011-05-09
1000023732xxxxx,1303537065,2011-04-22
1000022413xxxxx,1302326877,2011-04-08
1000022328xxxxx,1300810582,2011-03-22
1000019352xxxxx,1295628516,2011-01-21
1000014241xxxxx,1285972221,2010-10-01
1000013861xxxxx,1281882953,2010-08-15
1000014436xxxxx,1280116994,2010-07-25
1000012117xxxxx,1276055448,2010-06-08
1000010697xxxxx,1274090432,2010-05-17
1000010425xxxxx,1272438522,2010-04-28
1000008600xxxxx,1268201411,2010-03-09
1000008113xxxxx,1267667333,2010-03-03
1000006286xxxxx,1266618961,2010-02-19
1000006189xxxxx,1263726284,2010-01-17
1000006449xxxxx,1262406605,2010-01-01
1000003298xxxxx,1261112448,2009-12-17
1000005651xxxxx,1259793952,2009-12-02
1000005426xxxxx,1259605238,2009-11-30
1000005072xxxxx,1258400669,2009-11-16
1000004668xxxxx,1257502719,2009-11-06
1000002286xxxxx,1252567838,2009-09-10
1000001160xxxxx,1250562107,2009-08-17
1000001568xxxxx,1250382196,2009-08-15
</code></pre>
			<figcaption>
				<strong>Figure 1.</strong>
				Correlation between Facebook UID and Facebook Account Creation Date.
				(<a rel="enclosure" href="data/post/2013/03/18/inferring_fb_creation_date_from_uid_data/fbid_accountage.csv">download</a>)
			</figcaption>
		</figure>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-17T15:21:14-07:00">March 17, 2013</time>
			</div>
			<h1>Forensic Analysis of Phone Call Networks (Salvatore Catanese, Emilio Ferrara, Giacomo Fiumara)</h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



			<blockquote cite="http://arxiv.org/abs/cs/0703109">
				<p>
					In the context of preventing and fighting crime, the analysis of mobile phone traffic, among actors of a criminal network, is helpful in order to reconstruct illegal activities on the base of the relationships connecting those specific individuals. Thus, forensic analysts and investigators require new advanced tools and techniques which allow them to manage these data in a meaningful and efficient way. In this paper we present LogAnalysis, a tool we developed to provide visual data representation and filtering, statistical analysis features and the possibility of a temporal analysis of mobile phone activities. Its adoption may help in unveiling the structure of a criminal network and the roles and dynamics of communications among its components. By using LogAnalysis, forensic investigators could deeply understand hierarchies within criminal organizations, for example discovering central members that provide connections among different sub-groups, etc. Moreover, by analyzing the temporal evolution of the contacts among individuals, or by focusing on specific time windows they could acquire additional insights on the data they are analyzing. Finally, we put into evidence how the adoption of LogAnalysis may be crucial to solve real cases, providing as example a number of case studies inspired by real forensic investigations led by one of the authors.
				</p>
			</blockquote>
			<p>
				<a href="http://arxiv.org/abs/1303.1827">arXiv:1303.1827</a> [cs.SI]
			</p>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-15T07:38:12-07:00">March 15, 2013</time>
			</div>
			<h1><a href="post/2013/03/15/inferring_fb_creation_date_from_uid/">Inferring Facebook Account Creation Date From Facebook User ID</a></h1>
			<address
				 
					class="reblogged"
				 
			>
				 
					<a rel="bookmark canonical" href="http://metadatascience.com/2013/03/11/inferring-facebook-account-creation-date-from-facebook-user-id/">cross-blogged</a>
				 
				by <a href="http://twitter.com/accesstoken">accesstoken</a>
			</address>
			<div>



		<p>
			Calling the Facebook API is a (relatively) slow operation; especially if you have to call it multiple times.
			So, when possible, it is a good idea to get the information you need, without making API calls.
		</p>
		<p>
			Here I show you how to figure out the <em>creation date</em> of a Facebook account without having to call the Facebook API, just based on the user’s Facebook UID.
		</p>

		<h2>The Bad Way To Do It</h2>
		<p>
			As I explained in <a href="http://metadatascience.com/2013/03/05/how-to-estimate-the-facebook-account-creation-date/">my previous post</a>,
			it is possible to estimate the Facebook account creation date by retrieving the date of user’s oldest post.
			This method has a couple of draw backs:
		</p>
		<p>
			<strong>Draw Back #1</strong>:
			<img src="data/post/2013/03/15/inferring_fb_creation_date_from_uid/screenshot_read_stream.png" width="356" align="right" height="262" title="read_stream"/>
			You must have <em>‘read_stream’</em> permission which is an extended Facebook permission to read the user post stream.
			From a user’s point of view, this sounds scarier than the other basic permissions you probably ask for.
		</p>
		<p>
			<strong>Draw Back #2</strong>: As an extended permission it triggers a second permission screen that dramatically increases the <a href="http://en.wikipedia.org/wiki/User_experience">UX</a> friction for the users.
			(You want <em>low</em> friction UX.)
		</p>
		<p>
			<strong>Draw Back #3</strong>: The overhead of walking the entire post stream to determine age is very costly for the simple piece of information we synthesize.
			(You have to call the Facebook API over and over and over and over … again, since the post stream is paginated.
			I.e., this is at best an O(n) operation, where “n” relates to the user’s activity on Facebook.)
		</p>

		<h2>My Search For A Better Way</h2>
		<p>
			To overcome these issues I tried an to find an alternative, asynchronous approach.
			I was wondering if it is possible to estimate a Facebook account creation date by looking at Facebook User ID.
			I couldn’t find any official documentation on how Facebook generates a new Facebook user ID and how they are accomplishing that in a scalable fashion.
			One answer I could find was from <em>Jack Lindamood, Software Engineer at Facebook 2008-2012</em> which I found <a href="http://www.quora.com/Facebook-1/How-do-Facebook-use-incremented-IDs-for-both-users-and-Pages">here</a>:
		</p>
		<blockquote>
			<p>
				‘Lots’ of MySQL DBs.
				Each with their own unique number.
				Also, each has an autoincrement table.
				Then it’s just some math on the autoincrement value + unique_number * some_cap_per_db (it’s a bit more complicated due to special cases, but that’s pretty much how it works).
			</p>
		</blockquote>
		<p>
			Another explanation was from <em>Justin Mitchell, former engineering manager</em>.
			He explains <a href="http://www.quora.com/Facebook-Company-History/What-is-the-history-of-Facebooks-user-ID-numbering-system">here</a> the history of Facebook user ID numbering system:
		</p>
		<blockquote>
			<p>
				Facebook’s user ID schema reflects the history of the site as it transitioned from a single-server single-school operation to 400 million users. 
				User ID assignment has gone through several phases, notably:
			</p>
			<p>
				Harvard only.
				Facebook (or thefacebook.com, as it was called back then) was opened up to Harvard running off a single box that had mysql and apache.
				IDs were auto-incremented, starting at 4 (hi Zuck).
			</p>
			<p>
				Other schools.
				Other schools were initially completely separate sites, operating on their own boxes.
				IDs were still auto-increment per SQL box, but each server/school had a different prefix.
				For instance, all Columbia IDs are between 100000-199999 and all Stanford IDs are between 200000-299999.
				You can determine what school any early Facebook user attended based on his or her user ID.
			</p>
			<p>
				High schools.
				Someone must have figured out that this ID system didn’t scale very well, so Facebook changed its DB layout when high schools were introduced.
				While all the college users maintained their current DB, high school users were randomly assigned to one of many many high school DBs.
				These users IDs hash to the correct database, rather than simply being floor(ID / 100000).
			</p>
			<p>
				Open registration.
				Facebook maintained a similar layout once open reg was launched, except the new databases weren’t signified as “high school.”
			</p>
			<p>
				64 bit.
				Given Facebook’s growth rate, it was estimated that the entire world would be on the site by 2011, overflowing 32-bit space.
				While we considered limiting the site to the first 4-billion people to register and lobbying governments to reduce the world’s population, the growth team pushed pretty hard to just increase the ID space to 64-bit.
			</p>
		</blockquote>

		<h2>Using Facecbook UIDs For Predictions</h2>
		<p>
			So it seems that new Facebook IDs are 64 bits and contain 15 digits.
			There is a <a href="https://developers.facebook.com/blog/post/45/">post</a> dating from October 2007 that mentions that Facebook had plans to do this long time ago but according to this <a href="https://developers.facebook.com/blog/post/226/">post</a> from May, 2009, Facebook was going to release 64 bit user IDs back to 2009.
		</p>
		<p>
			I studied the correlation between Facebook User ID and Account Creation Date for a <em>tiny sample set</em> of 77 Facebook accounts.
			41 accounts of this sample set had a user ID containing 15 digits and for the rest the user ID has less than 15 digits.
			Figures below illustrate this correlation seperately for 64 bit UIDS (<em>figure 1</em>) and old style UIDs (<em>figure 2</em>).
		</p>
		<figure>
			<img src="data/post/2013/03/15/inferring_fb_creation_date_from_uid/fbid_age_15.png" width="390" height="390" title="uid==15"/>
			<figcaption>
				<strong>Figure 1.</strong>
			</figcaption>
		</figure>
		<figure>
			<img src="data/post/2013/03/15/inferring_fb_creation_date_from_uid/fbid_age_less.png" width="390" height="390" title="uid&lt;15"/>
			<figcaption>
				<strong>Figure 2.</strong>
			</figcaption>
		</figure>
		<p>
			The graph in <em>figure 1</em> is for the new(er) Facebook UIDs.
			The graph in <em>figure 2</em> is for the old style Facebook IDs.
			You can see that the correlation between Facebook UID and its <em>creation date</em> is <strong>a lot</strong> better for the new(er) Facebook UIDs than the old ones.
		</p>
		<p>
			Or in other words, as we observe, there is an interesting correlation between Facebook User ID and Account Creation Date for 64 bit user IDs (see <em>figure 1</em>).
			Also in this sample set, old UIDs are more than 800 days old (see <em>figure 2</em>).
			The overlap between two graphs might be a period that Facebook was moving from old UIDs to 64 bit ones.
		</p>
		<p>
			<strong>Therefore as an alternative approach to estimate the Facebook account creation date, we may leverage the monotonically increasing property of 64 bit Facebook user IDs and create a table of bounds that would give us at least a quarterly estimate on the creation date for the account - an appropriate level of granularity for this purpose.</strong>
			Taking this approach will reduce the number of permissions your application need and dramatically decrease the amount of processing time and remove a variable around the elapsed time to deliver a response.
		</p>
		<p>
			<span style="color:red"><b>Update (March 14, 2013):</b></span> 
			See <a href="http://metadatascience.com/2013/03/14/lookup-table-for-inferring-facebook-account-creation-date-from-facebook-user-id/">here</a> to download the data set.
		</p>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-14T06:44:41-07:00">March 14, 2013</time>
			</div>
			<h1><a href="post/2013/03/14/tag-cloud_drawing/">Tag-Cloud Drawing: Algorithms for Cloud Visualization (Owen Kaser, Daniel Lemire)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



			<blockquote cite="http://arxiv.org/abs/cs/0703109">
				<p>
					Tag clouds provide an aggregate of tag-usage statistics. They are typically sent as in-line HTML to browsers. However, display mechanisms suited for ordinary text are not ideal for tags, because font sizes may vary widely on a line. As well, the typical layout does not account for relationships that may be known between tags. This paper presents models and algorithms to improve the display of tag clouds that consist of in-line HTML, as well as algorithms that use nested tables to achieve a more general 2-dimensional layout in which tag relationships are considered. The first algorithms leverage prior work in typesetting and rectangle packing, whereas the second group of algorithms leverage prior work in Electronic Design Automation. Experiments show our algorithms can be efficiently implemented and perform well.
				</p>
			</blockquote>
			<p>
				<a href="http://arxiv.org/abs/cs/0703109">arXiv:cs/0703109</a> [cs.DS]
			</p>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-13T06:47:52-07:00">March 13, 2013</time>
			</div>
			<h1><a href="post/2013/03/13/fb_creation_date_estimate/">How to Estimate the Facebook Account Creation Date</a></h1>
			<address
				 
					class="reblogged"
				 
			>
				 
					<a rel="bookmark canonical" href="http://metadatascience.com/2013/03/05/how-to-estimate-the-facebook-account-creation-date/">cross-blogged</a>
				 
				by <a href="http://twitter.com/accesstoken">accesstoken</a>
			</address>
			<div>



		<p>
			Facebook <a href="http://developers.facebook.com/docs/reference/api/">Graph API</a> and <a href="http://developers.facebook.com/docs/reference/fql/">FQL</a> don’t provide you with a simple way of getting the creation date of a Facebook account.
			But if you have a valid Facebook <a href="http://developers.facebook.com/tools/explorer">Access Token</a> with <em>‘read_stream’</em> permission, it is possible to estimate the Facebook account creation date by finding the creation date of the oldest user post.
			According to the Facebook documentation,
		</p>
		<blockquote>
			<p>
				each query of the stream table is limited to the previous 30 days or 50 posts, whichever is greater, however you can use time-specific fields such as created_time along with FQL operators (such as &lt; or &gt;) to retrieve a much greater range of posts.
			</p>
		</blockquote>
		<p>
			Also you must have <em>‘read_stream’</em> permission:
		</p>
		<blockquote>
			<p>
				Querying without the <em>‘read_stream’</em> permission will return only the public view of the data (i.e. data that can be see when the user is logged out).
			</p>
		</blockquote>
		<p>
			Here is some code to do that:
		</p>
		<figure>
<pre><code>
/**
 * Estimating the Facebook account age by finding the creation date of the oldest post.
 * A valid Facebook Access Token with read_stream permission is required.
 *
 * @author Massoud Seifi, Ph.D. @ MetaDataScience.com
 */

class AccountAge
{

	public $baseUrl;

	function __construct()
	{
		$this-&gt;baseUrl = 'https://graph.facebook.com/';
	}

	/**
	 * Run a Facebook FQL query
	 * @param string $fql Facebook query 
	 * @param string $access_token Facebook Access Token
	 * @return array Return Facebook query result
	 */
	public function doFQLRequest($fql, $access_token)
	{
		$url = $this-&gt;baseUrl	. 'fql?q=' . urlencode($fql)
			. '&amp;access_token=' . $access_token;
		$ch = curl_init($url);
		curl_setopt($ch, CURLOPT_TIMEOUT, 60);
		curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
		curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, 0);

		$decodedResult = json_decode(curl_exec($ch), true);
		curl_close($ch);

		$result = array();
		if(isset($decodedResult['data']))
			$result = $decodedResult['data'];
		else
			throw new Exception("Facebook FQL Error. Please check if the access token is valid.\n");

		return $result;
	}

	/**
	 * Estimate the account age by finding the creation date of the oldest post
	 * @param string $access_token Facebook Access Token
	 * @return integer Return the Facebook account age in seconds
	 */
	public function getAccountAge($access_token)
	{
		$date = new \DateTime('now');
		$timestamp = $date-&gt;getTimestamp();
		echo "# Finding the oldest post may take several minutes to complete.\n";
		echo "# Please wait ";
		while (true){ // Loop until finding the oldest post
			echo ".";
			$fql = "SELECT created_time FROM stream WHERE source_id = me()"
				. " AND created_time &lt; " . $timestamp
				. " ORDER BY created_time ASC LIMIT 5000";
			$result = $this-&gt;doFQLRequest($fql, $access_token);
			if (!isset($result[0]['created_time']))
				break;
			$timestamp = $result[0]['created_time'];
		}
		echo "\n";
		$age = $date-&gt;getTimestamp() - $timestamp;
		return $age;
	}


	/**
	 * Display the account age in a human readable format
	 * @param int $age Account age in seconds
	 */
	public function printAccountAge($age)
	{
		$years = floor($age / (365*24*60*60));
		$months = floor(($age - $years * 365*24*60*60) / (30*24*60*60));
		$days = floor(($age - $years * 365*24*60*60 - $months * 30*24*60*60) / (24*60*60));
		echo "\nAccount age: $years years, $months months, $days days\n";	
	}

}

$p = new AccountAge();
// You need an Access Token with a read_stream permission
$access_token = 'AAACEdEose0cBAOY7bB3A9m7s3U6hbuJvfECxuZBFRN6YjqPC2eZB5x8WrnK51Gl3WsdwYovmxdPZCKFyJKB5TuFhpxsDJpAZCe9y6eutyQZDZD';
$age = $p-&gt;getAccountAge($access_token);
$p-&gt;printAccountAge($age);
</code></pre>
		</figure>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-12T07:14:33-07:00">March 12, 2013</time>
			</div>
			<h1><a href="post/2013/03/12/facebook_likes_as_a_predictor/">Private traits and attributes are predictable from digital records of human behavior (Michal Kosinskia, David Stillwell, and Thore Graepel)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



			<blockquote cite="http://arxiv.org/abs/cs/0703109">
				<p>
					We show that easily accessible digital records of behavior, Facebook Likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. The analysis presented is based on a dataset of over 58,000 volunteers who provided their Facebook Likes, detailed demographic profiles, and the results of several psychometric tests. The proposed model uses dimensionality reduction for preprocessing the Likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from Likes. The model correctly discriminates between homosexual and heterosexual men in 88% of cases, African Americans and Caucasian Americans in 95% of cases, and between Democrat and Republican in 85% of cases. For the personality trait “Openness,” prediction accuracy is close to the test–retest accuracy of a standard personality test. We give examples of associations between attributes and Likes and discuss implications for online personalization and privacy.
				</p>
			</blockquote>
			<p>
				<a href="http://www.pnas.org/content/early/2013/03/06/1218772110.abstract">PNAS 2013 : 1218772110v1-201218772</a>
			</p>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-03-11T07:32:53-07:00">March 11, 2013</time>
			</div>
			<h1><a href="post/2013/03/11/data_science_meetup_vancouver/">New Data Science Meetup Group In Vancouver</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



			<p>
					There is a new <a href="http://www.meetup.com/DataScience/">data science meetup group in Vancouver</a>.
					If you are in Vancouver and are into <em>data science</em>, then you should <a href="http://www.meetup.com/DataScience/">join this</a>.
			</p>
	



			</div>
			<div class="comments">
				no comments
			</div>
		</article>



		<div class="pagination">
			<nav>
				<ul>
					<li>
						<a href="age/by10/shift0/0.html">Older Page &#8827;</a>
					</li>
				</ul>
			<nav>
		</div>

		<footer>
			<a href="http://twitter.com/DataScholars"><img src="i/twitter.png" alit="twitter"></a>
		</footer>


		<script src="3/jquery/jquery-1.9.1.min.js"></script>
		<script src="3/typeset/src/linked-list.js"></script>
		<script src="3/typeset/src/linebreak.js"></script>
		<script src="3/typeset/lib/hypher.js"></script>
		<script src="3/typeset/lib/en-us.js"></script>
	</body>
</html>
