<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8" />
		<title>DataScholars - Data Science, Computer Science, Machine Learning, Artificial Intelligence, Computational Social Science, Data Mining, Analytics, Visualization</title>
		<link rel="stylesheet" href="../../../3/normalize/normalize.css" />
		<link rel="stylesheet" href="../../../screen.css" />
		<link rel="alternate" href="/feed/blog.rss" title="DataScholars" type="application/rss+xml" />

		<script src="../../../3/jquery/jquery-1.9.1.min.js"></script>
	</head>

	<body>
		<header>
			<h1><a href="http://datascholars.com/">DataScholars</a></h1>
			<nav>
				<ul>
					<li>
						<a href="../../../about.html">About</a>
					</li>
					<li>
						<a href="../../../subscribe.html">Subscribe</a>
					</li>
					<li>
						<a href="../../../sphere.html">Sphere</a>
					</li>
				</ul>
			</nav>
			<p class="blurb">
				A blog about
				<em>data science</em>,
				<em>computer science</em>,
				<em>machine learning</em>,
				<em>artificial intelligence</em>,
				<em>computational social science</em>,
				<em>data mining</em>,
				<em>analysis</em>,
				and
				<em>visualization</em>.
			</p>
		</header>

		<div class="pagination">
			<nav>
				<ul>
					<li>
						<a href="5.html">&#8826; Newer Page</a>
					</li>
					<li>
						<a href="3.html">Older Page &#8827;</a>
					</li>
				</ul>
			<nav>
		</div>







		<article>
			<div class="pubdate">
				<time datetime="2013-05-04T09:46:35-07:00">May 4, 2013</time>
			</div>
			<h1><a href="../../../post/2013/05/04/revealing_social_networks/">Revealing social networks of spammers through spectral clustering (Kevin S. Xu, Mark Kliger, Yilun Chen, Peter J. Woolf, Alfred O. Hero III)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://arxiv.org/abs/1305.0051">
			<p>
				To date, most studies on spam have focused only on the spamming phase of the spam cycle and have ignored the harvesting phase, which consists of the mass acquisition of email addresses.
				It has been observed that spammers conceal their identity to a lesser degree in the harvesting phase, so it may be possible to gain new insights into spammers' behavior by studying the behavior of harvesters, which are individuals or bots that collect email addresses.
				In this paper, we reveal social networks of spammers by identifying communities of harvesters with high behavioral similarity using spectral clustering.
				The data analyzed was collected through Project Honey Pot, a distributed system for monitoring harvesting and spamming.
				Our main findings are (1) that most spammers either send only phishing emails or no phishing emails at all, (2) that most communities of spammers also send only phishing emails or no phishing emails at all, and (3) that several groups of spammers within communities exhibit coherent temporal behavior and have similar IP addresses.
				Our findings reveal some previously unknown behavior of spammers and suggest that there is indeed social structure between spammers to be discovered.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1305.0051">arXiv:1305.0051</a> [cs.SI]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-26T07:08:21-07:00">April 26, 2013</time>
			</div>
			<h1><a href="../../../post/2013/04/26/jsnetworkx/">JSNetworkX: Visualizing Graphs On The Web Using JavaScript</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			Visualizing graphs (in the <em>graph theory</em> sense of the word) can be a challenge.
			Visualizing graphs on the web can be a even bigger challenge.
			Luckily there is <a href="http://felix-kling.de/JSNetworkX/">JSNetworkX</a>.
		</p>
		<p>
			JSNetworkX is an open source JavaScript library that makes visualizing graph data easy.
		</p>
		<p>
			JSNetwork is a port of the <a href="http://networkx.github.io/">NetworkX</a> graph visualization library 
			to JavaScript and the Web
			(using <a href="http://d3js.org/">D3</a>).
		</p>
		<p>
			Here is an example, as shown in <em>figure 1</em>.
		</p>
		<figure>
			<div id="jsnetworkx_20130426_1" style="border:1px solid black;height:220px;"></div>
			<script>
			$(document).ready(function() {

					var G = jsnx.Graph();
     
					G.add_nodes_from([1,2,3,4], {group:0});
					G.add_nodes_from([5,6,7], {group:1});
					G.add_nodes_from([8,9,10,11], {group:2});
     
					G.add_path([1,2,5,6,7,8,11]);
					G.add_edges_from([[1,3],[1,4],[3,4],[2,3],[2,4],[8,9],[8,10],[9,10],[11,10],[11,9]]);
     
					var color = d3.scale.category20();
					jsnx.draw(G, {
						element: '#jsnetworkx_20130426_1',
						layout_attr: {
							charge: -120,
							linkDistance: 20
						},
						node_attr: {
							r: 5,
							title: function(d) { return d.label;}
						},
						node_style: {
							fill: function(d) {
								return color(d.data.group);
							},
							stroke: 'none'
						},
						edge_style: {
						fill: '#999'
					}
				});

			});
			</script>
			<figcaption>
				<b>Figure 1.</b>
				Sample graph visualization created with JSNetworkX.
			</figcaption>
		</figure>
		<p>
			And here is the code for the graph visualization in <em>figure 1</em>, as shown in <em>figure 2</em>.
		</p>
		<figure>
			<pre><code>
var G = jsnx.Graph();
     
G.add_nodes_from([1,2,3,4], {group:0});
G.add_nodes_from([5,6,7], {group:1});
G.add_nodes_from([8,9,10,11], {group:2});
     
G.add_path([1,2,5,6,7,8,11]);
G.add_edges_from([[1,3],[1,4],[3,4],[2,3],[2,4],[8,9],[8,10],[9,10],[11,10],[11,9]]);
     
var color = d3.scale.category20();
jsnx.draw(G, {
  element: '#PUT_IT_OF_WHERE_TO_RENDER_THE_GRAPH_HERE',
  layout_attr: {
    charge: -120,
    linkDistance: 20
  },
  node_attr: {
    r: 5,
    title: function(d) { return d.label;}
  },
  node_style: {
    fill: function(d) {
      return color(d.data.group);
    },
    stroke: 'none'
  },
  edge_style: {
    fill: '#999'
  }
});
			</code></pre>
			<figcaption>
				<b>Figure 2.</b>
				JavaScript source code using JSNetworkX for the graph visualization shown in <em>figure 1</em>.
			</figcaption>
		</figure>
		<p>
			More information about
			<a href="http://felix-kling.de/JSNetworkX/">JSNetworkX</a>
			is available
			<a href="http://felix-kling.de/JSNetworkX/">here</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-24T07:17:48-07:00">April 24, 2013</time>
			</div>
			<h1><a href="../../../post/2013/04/24/networks_crowds_and_markets/">Networks, Crowds, and Markets: Reasoning About a Highly Connected World (David Easley, Jon Kleinberg)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<img src="../../../../data/post/2013/04/24/networks_crowds_and_markets/networks_crowds_and_markets.jpg" align="right" style="width:240px;"/>
			<a href="http://www.arts.cornell.edu/econ/deasley/">David Easley</a>
			and
			<a href="http://www.cs.cornell.edu/home/kleinber/">Jon Kleinberg</a>
			have created an <i>excellent</i> book on (a subset of graph theory known as) small-world networks and their applications, such as social networks.
			Although that description probably doesn't do the book justice.
			(The book is definitely worth a read.)
		</p>
		<p>
			It is called:
			<a href="http://www.cs.cornell.edu/home/kleinber/networks-book/">Networks, Crowds, and Markets: Reasoning About a Highly Connected World</a>.
		</p>
		<p>
			You can either <a href="http://www.cambridge.org/us/networksbook">buy a copy here</a>
			or <a href="http://www.cs.cornell.edu/home/kleinber/networks-book/">download it for free</a>.
		</p>
		<p>
			Here is their description of it:
		</p>
		<blockquote cite="http://www.cs.cornell.edu/home/kleinber/networks-book/">
			<p>
				Over the past decade there has been a growing public fascination with the complex "connectedness" of modern society. This connectedness is found in many incarnations: in the rapid growth of the Internet and the Web, in the ease with which global communication now takes place, and in the ability of news and information as well as epidemics and financial crises to spread around the world with surprising speed and intensity. These are phenomena that involve networks, incentives, and the aggregate behavior of groups of people; they are based on the links that connect us and the ways in which each of our decisions can have subtle consequences for the outcomes of everyone else.
			</p>
			<p>
				<i>Networks, Crowds, and Markets</i> combines different scientific perspectives in its approach to understanding networks and behavior. Drawing on ideas from economics, sociology, computing and information science, and applied mathematics, it describes the emerging field of study that is growing at the interface of all these areas, addressing fundamental questions about how the social, economic, and technological worlds are connected.
			</p>
		</blockquote>
		<p>
			[<a href="http://www.cs.cornell.edu/home/kleinber/networks-book/">HTML + PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-22T07:28:02-07:00">April 22, 2013</time>
			</div>
			<h1><a href="../../../post/2013/04/22/analysing_mood_patterns/">Analysing Mood Patterns in the United Kingdom through Twitter Content (Vasileios Lampos, Thomas Lansdall-Welfare, Ricardo Araya, Nello Cristianini)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://arxiv.org/abs/1304.5507">
			<p>
				Social Media offer a vast amount of geo-located and time-stamped textual content directly generated by people.
				This information can be analysed to obtain insights about the general state of a large population of users and to address scientific questions from a diversity of disciplines.
				In this work, we estimate temporal patterns of mood variation through the use of emotionally loaded words contained in Twitter messages, possibly reflecting underlying circadian and seasonal rhythms in the mood of the users.
				We present a method for computing mood scores from text using affective word taxonomies, and apply it to millions of tweets collected in the United Kingdom during the seasons of summer and winter.
				Our analysis results in the detection of strong and statistically significant circadian patterns for all the investigated mood types.
				Seasonal variation does not seem to register any important divergence in the signals, but a periodic oscillation within a 24-hour period is identified for each mood type.
				The main common characteristic for all emotions is their mid-morning peak, however their mood score patterns differ in the evenings.
			</p>
		</blockquote>
		<p>
			<a href="http://arxiv.org/abs/1304.5507">arXiv:1304.5507</a> [cs.SI]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-21T09:45:23-07:00">April 21, 2013</time>
			</div>
			<h1><a href="../../../post/2013/04/21/plos_text_mining_collection/">New: PLOS Text Mining</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			The folks over at <a href="http://www.plos.org/">PLOS</a> are introducing the <a href="http://www.ploscollections.org/textmining">PLOS Text Mining Collection</a>.
		</p>
		<blockquote cite="http://blogs.plos.org/everyone/2013/04/17/announcing-the-plos-text-mining-collection/">
			<p>
				Text Mining is an interdisciplinary field combining techniques from linguistics, computer science and statistics to build tools that can efficiently retrieve and extract information from digital text.
				Over the last few decades, there has been increasing interest in text mining research because of the potential commercial and academic benefits this technology might enable.
			</p>
			<p>
				[...]
			</p>
			<p>
				First, the rate of growth of the scientific literature has now outstripped the ability of individuals to keep pace with new publications, even in a restricted field of study.
				Second, text-mining tools have steadily increased in accuracy and sophistication to the point where they are now suitable for widespread application.
				Finally, the rapid increase in availability of digital text in an Open Access format now permits text-mining tools to be applied more freely than ever before.
			</p>
			<p>
				[...]
			</p>
			<p>
				PLOS launches the <a href="http://www.ploscollections.org/textmining">Text Mining Collection</a>, a compendium of major reviews and recent highlights published in the PLOS family of journals on the topic of text mining.
				As one of the major publishers of the Open Access scientific literature, it is perhaps no coincidence that research in text mining in PLOS journals is flourishing.
				As noted above, the widespread application and societal benefits of text mining is most easily achieved under an Open Access model of publishing, where the barriers to obtaining published articles are minimized and the ability to remix and redistribute data extracted from text is explicitly permitted.
				Furthermore, PLOS is one of the few publishers who is actively promoting text mining research by providing an open <a href="http://api.plos.org/">Application Programming Interface</a> to mine their journal content.
			</p>
		</blockquote>
		<p>
			See it <a href="http://www.ploscollections.org/textmining">here</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-19T06:15:44-07:00">April 19, 2013</time>
			</div>
			<h1><a href="../../../post/2013/04/19/information_theory_pattern_recognition_and_neural_networks/">Video Lectures: Information Theory, Pattern Recognition, and Neural Networks, by David J. C. MacKay</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			<a href="http://www.inference.phy.cam.ac.uk/mackay/">David J.C. MacKay</a> has a number of
			<a href="http://videolectures.net/course_information_theory_pattern_recognition/">video lectures</a>
			available on:
			<a href="http://videolectures.net/course_information_theory_pattern_recognition/">Information Theory, Pattern Recognition, and Neural Networks</a>.
		</p>
		<p>
			Here is the complete list of video lectures:
		</p>
		<ul>
			<li>
				<a href="http://videolectures.net/mackay_course_01/">Lecture 1: Introduction to Information Theory</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_02/">Lecture 2: Entropy and Data Compression (I): Introduction to Compression, Information Theory and Entropy</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_03/">Lecture 3: Entropy and Data Compression (II): Shannon's Source Coding Theorem and the Bent Coin Lottery</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_04/">Lecture 4: Entropy and Data Compression (III): Shannon's Source Coding Theorem, Symbol Codes</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_05/">Lecture 5: Entropy and Data Compression (IV): Shannon's Source Coding Theorem, Symbol Codes and Arithmetic Coding</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_06/">Lecture 6: Noisy Channel Coding (I): Inference and Information Measures for Noisy Channels</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_07/">Lecture 7: Noisy Channel Coding (II): The Capacity of a Noisy Channel</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_08/">Lecture 8: Noisy Channel Coding (III): The Noisy-Channel Coding Theorem</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_09/">Lecture 9: A Noisy Channel Coding Gem, And An Introduction To Bayesian Inference (I)</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_10/">Lecture 10: An Introduction To Bayesian Inference (II): Inference Of Parameters And Models</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_11/">Lecture 11: Approximating Probability Distributions (I): Clustering As An Example Inference Problem</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_12/">Lecture 12: Approximating Probability Distributions (II): Monte Carlo Methods (I): Importance Sampling, Rejection Sampling, Gibbs Sampling, Metropolis Method</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_13/">Lecture 13: Approximating Probability Distributions (III): Monte Carlo Methods (II): Slice Sampling, Hybrid Monte Carlo, Over-relaxation, Exact Sampling </a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_14/">Lecture 14: Approximating Probability Distributions (IV): Variational Methods</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_15/">Lecture 15: Data Modelling With Neural Networks (I): Feedforward Networks: The Capacity Of A Single Neuron, Learning As Inference</a>
			</li>
			<li>
				<a href="http://videolectures.net/mackay_course_16/">Lecture 16: Data Modelling With Neural Networks (II): Content-Addressable Memories And State-Of-The-Art Error-Correcting Codes</a>
			</li>
		</ul>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-18T07:02:36-07:00">April 18, 2013</time>
			</div>
			<h1><a href="../../../post/2013/04/18/hadley_alexander_wickham_vancouver/">Hadley Alexander Wickham: Speaking at Vancouver Meetup on Wednesday May 8th at 7:00 PM</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			For those who use <a href="http://www.r-project.org/">R</a>, the name
			<a href="http://had.co.nz/">Hadley Alexander Wickham</a> is a well known one.
		</p>
		<p>
			Hadley is coming to Vancouver, and will be speaking at a combined meetup event for the 
			Vancouver-based <a href="http://www.meetup.com/DataScience/events/114687772/">Data Science group</a>
			and the <a href="http://www.meetup.com/Vancouver-R-Users-Group-data-analysis-statistics/events/114670912/">Vancouver R user group</a>.
		</p>
		<p>
			If you are in the Vancouver area, use R, or are interested in statistics or visualization, you should be there.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-17T07:27:54-07:00">April 17, 2013</time>
			</div>
			<h1><a href="../../../post/2013/04/17/tools_for_exploring_data_and_models/">Practical Tools For Exploring Data And Models (Hadley Alexander Wickham)</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<blockquote cite="http://had.co.nz/thesis/">
			<p>
				This thesis describes three families of tools for exploring data and models.
				It is organised in roughly the same way that you perform a data analysis.
				First, you get the data in a form that you can work with.
				Chapter 2 describes the reshape framework for restructuring data.
				Second, you plot the data to get a feel for what is going on.
				Chapter 3 introduces the layered grammar of graphics.
				Third, you iterate between graphics and models to build a succinct quantitative summary of the data.
				Chapter 4 introduces some strategies for visualising models.
				Finally, you look back at what you have done, and contemplate what tools you need to do better in the future.
				Chapter 5 summarises the impact of my work and my plans for the future.
			</p>
		</blockquote>
		<p>
			[<a href="http://had.co.nz/thesis/practical-tools-hadley-wickham.pdf">PDF</a>]
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-16T07:04:17-0700">April 16, 2013</time>
			</div>
			<h1><a href="../../../post/2013/04/16/spark_cli_graphs/">Sprak: Graph Visualization From The Command Line</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			So you are at the command line.
			You have a bunch of data you pulled from the database or a file.
			You did a bunch of "magic" to <code>awk</code>, <code>sort</code> and other command line tools to extract the "important" parts of the data.
		</p>
		<p>
			Now you want to see that "important" extract in a graph.
		</p>
		<p>
			What's the fastest way to visualize it?
			What's the fastest to see a graph?
		</p>
		<p>
			MS Excel?
			OpenOffice Calc?
			GNU Octave?
			R?
		</p>
		<p>
			No, no, no and no.
		</p>
		<p>
			The fastest way is using  <a href="http://zachholman.com/spark/">spark</a>.
		</p>
		<p>
			 <a href="http://zachholman.com/spark/">Spark</a>  lets you create and view graphs right from the command line.
		</p>
		<p>
			Here is a very basic example:
		</p>
		<figure>
			<figcaption>
				<pre><code>
spark 0 30 55 80 33 150
				</code></pre>
				<strong>Figure 1.</strong>
				Very basic usage of <code>spak</code>.
			</figcaption>
		</figure>
		<figure>
			<figcaption>
				<pre><code>
▁▂▃▅▂▇
				</code></pre>
				<strong>Figure 2.</strong>
				Output of <code>spark</code> command in <em>figure 1</em>.
			</figcaption>
		</figure>
		<p>
			And here is a more typical example:
		</p>
		<figure>
			<figcaption>
				<pre><code>
curl http://earthquake.usgs.gov/earthquakes/catalogs/eqs1day-M1.txt --silent | sed '1d' | cut -d, -f9 | spark
				</code></pre>
				<strong>Figure 3.</strong>
				More typical usage of <code>spak</code>.
				Magnitude of earthquakes over 1.0 in the last 24 hours.
			</figcaption>
		</figure>
		<figure>
			<figcaption>
				<pre><code>
 ▅▆▂▃▂▂▂▅▂▂▅▇▂▂▂▃▆▆▆▅▃▂▂▂▁▂▂▆▁▃▂▂▂▂▃▂▆
				</code></pre>
				<strong>Figure 4.</strong>
				Output of <code>spark</code> command in <em>figure 3</em>.
			</figcaption>
		</figure>
		<p>
			(<a href="https://github.com/holman/spark/wiki/Wicked-Cool-Usage">More spark examples here</a>.)
		</p>
		<p>
			Check out <a href="http://zachholman.com/spark/">spark</a>.
		</p>
	



			</div>
		</article>







		<article>
			<div class="pubdate">
				<time datetime="2013-04-15T06:59:44-0700">April 15, 2013</time>
			</div>
			<h1><a href="../../../post/2013/04/15/ontario_open_data/">We Love Open Data: Ontario Open Data</a></h1>
			<address
				 
			>
				 
				by <a href="http://twitter.com/reiver">reiver</a>
			</address>
			<div>



		<p>
			The "currency" of a <em>data scientist</em>'s vocation is (surprise, surprise) <em>data</em>.
			Sometimes data scientists have to go to great lengths to gather the data sets themselves.
			And sometimes people will "give" it to them. 
		</p>
		<p>
			The government of Ontario has done just that.
		</p>
		<p>
			Meet the <a href="http://www.ontario.ca/government/government-ontario-open-data">Ontario Open Data portal</a>.
		</p>
		<p>
			Explore the data sets they offer yourself.
		</p>
	



			</div>
		</article>






		<div class="pagination">
			<nav>
				<ul>
					<li>
						<a href="5.html">&#8826; Newer Page</a>
					</li>
					<li>
						<a href="3.html">Older Page &#8827;</a>
					</li>
				</ul>
			<nav>
		</div>

		<footer>
			<a href="http://twitter.com/DataScholars"><img src="../../../i/twitter.png" alit="twitter"></a>
		</footer>


		<script src="../../../3/jquery/jquery-1.9.1.min.js"></script>
		<script src="../../../3/typeset/src/linked-list.js"></script>
		<script src="../../../3/typeset/src/linebreak.js"></script>
		<script src="../../../3/typeset/lib/hypher.js"></script>
		<script src="../../../3/typeset/lib/en-us.js"></script>

		<script src="../../../3/d3/d3.v3.min.js"></script>
		<script src="../../../3/jsnetworkx/jsnetworkx.js"></script>

		<script src="../../../webstats.js"></script>
	</body>
</html>
